% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lightgbm.R
\name{lightgbm}
\alias{lightgbm}
\alias{lightgbm.formula}
\alias{lightgbm.data.frame}
\alias{lightgbm.matrix}
\alias{lightgbm.dgCMatrix}
\title{Train a LightGBM model}
\usage{
lightgbm(...)

\method{lightgbm}{formula}(
  formula,
  data,
  weights = NULL,
  init_score = NULL,
  objective = "auto",
  nrounds = 100L,
  nthreads = parallel::detectCores(),
  params = list(),
  dataset_params = list(),
  verbose = 1L,
  eval_freq = 1L,
  early_stopping_rounds = NULL,
  save_name = NULL,
  serializable = TRUE,
  ...
)

\method{lightgbm}{data.frame}(
  data,
  label,
  weights = NULL,
  init_score = NULL,
  objective = "auto",
  nrounds = 100L,
  nthreads = parallel::detectCores(),
  params = list(),
  dataset_params = list(),
  verbose = 1L,
  eval_freq = 1L,
  early_stopping_rounds = NULL,
  save_name = NULL,
  serializable = TRUE,
  ...
)

\method{lightgbm}{matrix}(
  X,
  y,
  weights = NULL,
  init_score = NULL,
  objective = "auto",
  nrounds = 100L,
  nthreads = parallel::detectCores(),
  params = list(),
  dataset_params = list(),
  verbose = 1L,
  eval_freq = 1L,
  early_stopping_rounds = NULL,
  save_name = NULL,
  serializable = TRUE,
  ...
)

\method{lightgbm}{dgCMatrix}(
  X,
  y,
  weights = NULL,
  init_score = NULL,
  objective = "auto",
  nrounds = 100L,
  nthreads = parallel::detectCores(),
  params = list(),
  dataset_params = list(),
  verbose = 1L,
  eval_freq = 1L,
  early_stopping_rounds = NULL,
  save_name = NULL,
  serializable = TRUE,
  ...
)
}
\arguments{
\item{...}{Additional arguments passed to \code{\link{lgb.train}}. For example:
\itemize{
   \item{\code{valids}: a list of \code{lgb.Dataset} objects, used for validation}
   \item{\code{eval}: evaluation function, can be (a list of) character or custom eval function}
   \item{\code{record}: Boolean, TRUE will record iteration message to \code{booster$record_evals}}
   \item{\code{colnames}: feature names, if not null, will use this to overwrite the names in dataset}
   \item{\code{categorical_feature}: categorical features. This can either be a character vector of feature
         names or an integer vector with the indices of the features (e.g. \code{c(1L, 10L)} to
         say "the first and tenth columns"). This parameter is not supported in the `formula` and
         `data.frame` interfaces.}
   \item{\code{reset_data}: Boolean, setting it to TRUE (not the default value) will transform the booster model
                     into a predictor model which frees up memory and the original datasets}
}}

\item{formula}{A formula for specifying the response/label and predictors/features in the
               model to be fitted. This is provided for ease of use, but using the `formula` interface
               is discouraged for a couple reasons (see details section for mode details):\itemize{
               \item It converts all factor variables to dummy encoding, which typically does not lead to
                     models as good as those in which categorical variables are treated as such.
               \item It uses base R's formula handling for inputs, which can be particularly
                     computationally inefficient compared to the alternatives.
               \item If the number of variables is large, it can increase model size quite a bit.
               }

               If using the `formula` interface, then `data` must be a `data.frame`.}

\item{data}{A `data.frame`. In the non-formula interface, it will use all available variables
            (those not specified as being `label`, `weight`, or `init_score`) as features / predictors,
            and will assume their types are:\itemize{
            \item Numeric, if they are of type `numeric`, `integer`, `Date`, `POSIXct`.
            \item Categorical, if they are of type `factor`, `character`.
            }

            Other variable types are not accepted. Note that the underlying core library only accepts
            `numeric` inputs, thus other types will end up being casted.

            Note that, if using the `data.frame` interface, it is not possible to manually specify
            categorical variables through `params` - instead, these will be deduced from the data types,
            and their encoding will be handled internally in the fitting and prediction functions.
            Under the `data.frame` interface, if the data contains any categorical variables, then at
            prediction time only `data.frame` inputs will be allowed.}

\item{weights}{Sample / observation weights for rows in `X` / `data`. Same format as
`y` (i.e. accepts non-standard evaluation for column names, and accepts numeric vectors).}

\item{init_score}{Initial values for each observation from which the boosting process will
be started (e.g. as the result of some previous model). If not passing it (the default),
will start from a blank state.}

\item{objective}{Optimization objective (e.g. `"regression"`, `"binary"`, etc.).
                 For a list of accepted objectives, see
                 \href{https://lightgbm.readthedocs.io/en/latest/Parameters.html}{
                 the "Parameters" section of the documentation}.

                 If passing `"auto"`, will be deduced from the type of `y` / `label`:\itemize{
                 \item If `y` is not a factor, will set the objective to `"regression"`.
                 \item If `y` is a factor with two classes, will set the objective to `"binary"`.
                 \item If `y` is a factor with more than two classes, will set the objective to `"multiclass"`.
                 }

                 If `y` is a factor, then it will automatically set parameter `num_classes` based on
                 its number of levels, overriding any such entry in `params` if it is present there.}

\item{nrounds}{number of training rounds}

\item{nthreads}{Number of parallel threads to use. For best speed, this should be set to the number of
                physical cores in the CPU - in a typical x86-64 machine, this corresponds to half the
                number of maximum threads (e.g. `nthreads = max(parallel::detectCores() / 2L, 1L)` as
                a shorthand for the optimal value).

                Be aware that using too many threads can result in speed degradation in smaller datasets
                (see the parameters documentation for more details).

                If passing zero, will use the default number of threads configured for OpenMP.
                
                This parameter overrides `num_threads` in `params` if it exists there.}

\item{params}{a list of parameters. See \href{https://lightgbm.readthedocs.io/en/latest/Parameters.html}{
the "Parameters" section of the documentation} for a list of parameters and valid values.}

\item{dataset_params}{Extra parameters to pass to \link{lgb.Dataset} once it comes the
                      time to convert the dataset to this library's internal format.

                      For a list of the accepted parameters, see
                      \href{https://lightgbm.readthedocs.io/en/latest/Parameters.html#io-parameters}{
                      the "I/O Parameters" section of the documentation}.}

\item{verbose}{verbosity for output, if <= 0, also will disable the print of evaluation during training}

\item{eval_freq}{evaluation output frequency, only effect when verbose > 0}

\item{early_stopping_rounds}{int. Activates early stopping. When this parameter is non-null,
training will stop if the evaluation of any metric on any validation set
fails to improve for \code{early_stopping_rounds} consecutive boosting rounds.
If training stops early, the returned model will have attribute \code{best_iter}
set to the iteration number of the best iteration.}

\item{save_name}{File name to use when writing the trained model to disk. Should end in ".model".
If passing `NULL`, will not save the trained model to disk.}

\item{serializable}{whether to make the resulting objects serializable through functions such as
\code{save} or \code{saveRDS} (see section "Model serialization").}

\item{X}{Data features / covariates / predictors with which the model will try to predict `y`.

         Note that, if using non-standard evaluation for `y`, `weights`, or `init_score` (specifying
         them as column names from `X`), then `X` will be subsetted, and any additional parameters
         passed that correspond to column indices (such as per-column `max_bin` or
         `categorical_features`) will be applied on the subsetted data.
         
         Supports dense matrices from base R (class `matrix`, will be casted to `double` storage
         mode if it isn't already) and sparse matrices in CSC format from the `Matrix` package
         (class `dgCMatrix`).}

\item{y, label}{Target / response variable to predict. May be passed as:\itemize{
               \item The name of a column from `X` / `data`, if it has column names. Will use non-standard
                     evaluation in order to try to determine if it matches with the name of a column in
                     `X` / `data` (i.e. will accept it as the name of a column without putting quotes
                     around it), and can also be passed as a character.
                \item A vector with the number of entries matching to the number of rows in `X` / `data`.
                }
                If passing `objective="auto"`, the optimization objective will be determined according to
                the type / class of this variable.

                If `y` is passed as a factor, then `num_class` in `params` will be set automatically
                according to its levels.

                Passing `y` as a factor will also make \link{predict.lgb.Booster} use its levels in the
                outputs from predictions when appropriate.}
}
\value{
A trained \code{lgb.Booster} model object.
}
\description{
Simplified interface for training / fitting a LightGBM model which follows typical
             R idioms for model fitting and predictions. Note that this interface does not
             expose the full spectrum of library features as \link{lgb.train} does.
}
\details{
This is a thin wrapper over \link{lgb.Dataset} and then \link{lgb.train} which performs
         extra steps such as  automatically detecting categorical variables and handling their
         encoding. It is intended as an easy-to-use interface that follows common R idioms for
         predictive models.

         It uses base R's functions for processing the data, such as `factor`, which are not
         particularly efficient - for serious usage, it is recommended to use the \link{lgb.train}
         interface with \link{lgb.Dataset} instead, handling aspects such as encoding of categorical
         variables externally through your favorite tools.

         \bold{Important:} using the `formula` interface relies on R's own formula handling, which
         might be very slow for large inputs and will dummy-encode all categorical variables
         (meaning: they will not be treated as categorical in tree splits, rather each level will be
         treated as a separate variable, without exploiting the sparsity and independence patterns
         in the encoded data).

         When models are produced through this interface (as opposed to \link{lgb.train}), the
         method \link{predict.lgb.Booster} will additionally gain new behaviors, such as taking
         columns by name from the new input data or adding names to the resulting predicted matrices
         (based on the classes or features depending on what is being predicted).
}
\section{Early Stopping}{


         "early stopping" refers to stopping the training process if the model's performance on a given
         validation set does not improve for several consecutive iterations.

         If multiple arguments are given to \code{eval}, their order will be preserved. If you enable
         early stopping by setting \code{early_stopping_rounds} in \code{params}, by default all
         metrics will be considered for early stopping.

         If you want to only consider the first metric for early stopping, pass
         \code{first_metric_only = TRUE} in \code{params}. Note that if you also specify \code{metric}
         in \code{params}, that metric will be considered the "first" one. If you omit \code{metric},
         a default metric will be used based on your choice for the parameter \code{obj} (keyword argument)
         or \code{objective} (passed into \code{params}).
}

\examples{
library(lightgbm)
data("iris")
model <- lightgbm(Species ~ ., data = iris, verbose = -1L, nthreads = 1L)
pred <- predict(model, iris, type = "class")
all(pred == iris$Species)

model <- lightgbm(iris, Species, verbose = -1L, nthreads = 1L)
head(predict(model, iris, type = "score"))

model <- lightgbm(as.matrix(iris[, -5L]), iris$Species, verbose = -1L, nthreads = 1L)
head(predict(model, iris, type = "raw"))
}
