{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report_date</th>\n",
       "      <th>amount</th>\n",
       "      <th>is_workday</th>\n",
       "      <th>day_of_the_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170322</td>\n",
       "      <td>128889919</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170324</td>\n",
       "      <td>123863161</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170326</td>\n",
       "      <td>104586428</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170331</td>\n",
       "      <td>130570437</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170402</td>\n",
       "      <td>156932119</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>20191102</td>\n",
       "      <td>578548087</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>20191106</td>\n",
       "      <td>607952700</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>20191108</td>\n",
       "      <td>620204346</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>20191109</td>\n",
       "      <td>647024387</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>20191112</td>\n",
       "      <td>559582431</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>973 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     report_date     amount  is_workday  day_of_the_week\n",
       "0       20170322  128889919           1                2\n",
       "1       20170324  123863161           1                4\n",
       "2       20170326  104586428           0                6\n",
       "3       20170331  130570437           1                4\n",
       "4       20170402  156932119           0                6\n",
       "..           ...        ...         ...              ...\n",
       "968     20191102  578548087           0                5\n",
       "969     20191106  607952700           1                2\n",
       "970     20191108  620204346           1                4\n",
       "971     20191109  647024387           0                5\n",
       "972     20191112  559582431           1                1\n",
       "\n",
       "[973 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./raw_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictday 1\n"
     ]
    }
   ],
   "source": [
    "df['report_date'] = df['report_date'].astype(str)\n",
    "df = df.sort_values('report_date')\n",
    "\n",
    "def precision_compute(pred_num, real_num):\n",
    "    if real_num == 0:\n",
    "        return 1.0\n",
    "    return 1 - np.abs(pred_num - real_num) / real_num\n",
    "\n",
    "results = []\n",
    "predictday = 0\n",
    "print(\"predictday\",predictday + 1)\n",
    "df_feature = df.copy()\n",
    "df_feature['report_date2'] = pd.to_datetime(df_feature['report_date'])\n",
    "df_feature['day'] = df_feature['report_date2'].map(lambda x:x.day)\n",
    "df_feature['month'] = df_feature['report_date2'].map(lambda x:x.month)\n",
    "df_feature['year'] = df_feature['report_date2'].map(lambda x:x.year)\n",
    "df_feature['is_workday'] = df_feature['is_workday'].astype(int)\n",
    "df_feature['day_of_the_week'] = df_feature['day_of_the_week'].astype(int)\n",
    "df_feature['amount'] = df_feature['amount'].map(lambda x:int(x)/10**5).astype(int)\n",
    "del df_feature['report_date2']\n",
    "\n",
    "cache = df_feature[['report_date','amount','is_workday']].values\n",
    "\n",
    "amounts = []\n",
    "\n",
    "amount_label2 = []\n",
    "workday = 0\n",
    "\n",
    "last_amount = []\n",
    "min_amount_3 = []\n",
    "max_amount_3 = []\n",
    "mean_amount_3 = []\n",
    "std_amount_3 = []\n",
    "min_amount_7 = []\n",
    "max_amount_7 = []\n",
    "mean_amount_7 = []\n",
    "std_amount_7 = []\n",
    "min_amount_14 = []\n",
    "max_amount_14 = []\n",
    "mean_amount_14 = []\n",
    "std_amount_14 = []\n",
    "min_amount_30 = []\n",
    "max_amount_30 = []\n",
    "mean_amount_30 = []\n",
    "std_amount_30 = []\n",
    "continueworkday = []\n",
    "\n",
    "amounts.append(cache[0,1])\n",
    "for i in range(predictday):  # padding\n",
    "    amounts.append(cache[0,1])\n",
    "\n",
    "for i in range(cache.shape[0]):\n",
    "    amount = cache[i,1]\n",
    "    if cache[i,2] == 1:\n",
    "        workday += 1\n",
    "    else:\n",
    "        workday = 0\n",
    "    continueworkday.append(workday)\n",
    "\n",
    "\n",
    "    # sliding window\n",
    "    for length in [3,7,14,30]:\n",
    "        if predictday != 0:\n",
    "            l = amounts[-length:-predictday]\n",
    "        else:\n",
    "            l = amounts[-length:]\n",
    "        if len(l) == 0:\n",
    "            l = [amount]\n",
    "        eval('min_amount_' + str(length)).append(min(l))\n",
    "        eval('max_amount_' + str(length)).append(max(l))\n",
    "        eval('mean_amount_' + str(length)).append(np.mean(l))\n",
    "        eval('std_amount_' + str(length)).append(np.std(l))\n",
    "    amount_label2.append(amount / amounts[-1 - predictday])\n",
    "    last_amount.append(amounts[-1 - predictday])\n",
    "    amounts.append(amount)\n",
    "\n",
    "df_feature['amount2'] = np.array(amount_label2)\n",
    "df_feature['last_amount'] = np.array(last_amount)\n",
    "for length in [3,7,14,30]:\n",
    "    df_feature['min_amount_' + str(length)] = np.array(eval('min_amount_' + str(length)))\n",
    "    df_feature['max_amount_' + str(length)] = np.array(eval('max_amount_' + str(length)))\n",
    "    df_feature['mean_amount_' + str(length)] = np.array(eval('mean_amount_' + str(length)))\n",
    "    df_feature['std_amount_' + str(length)] = np.array(eval('std_amount_' + str(length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 90.6070 - val_loss: 100.0000\n",
      "Epoch 2/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 83.0337 - val_loss: 100.0000\n",
      "Epoch 3/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 80.4516 - val_loss: 57.4738\n",
      "Epoch 4/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 78.3150 - val_loss: 65.9719\n",
      "Epoch 5/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 75.3061 - val_loss: 79.8132\n",
      "Epoch 6/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 71.9679 - val_loss: 56.8308\n",
      "Epoch 7/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 71.2557 - val_loss: 57.2107\n",
      "Epoch 8/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 70.0611 - val_loss: 65.5547\n",
      "Epoch 9/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 68.3002 - val_loss: 55.1020\n",
      "Epoch 10/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 69.0603 - val_loss: 57.6169\n",
      "Epoch 11/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 68.0582 - val_loss: 54.9165\n",
      "Epoch 12/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 65.5051 - val_loss: 54.4025\n",
      "Epoch 13/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 64.5987 - val_loss: 54.1885\n",
      "Epoch 14/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 64.4134 - val_loss: 55.1623\n",
      "Epoch 15/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 64.0774 - val_loss: 53.7522\n",
      "Epoch 16/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 63.3811 - val_loss: 53.2614\n",
      "Epoch 17/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 61.5492 - val_loss: 54.0117\n",
      "Epoch 18/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 61.5592 - val_loss: 55.9223\n",
      "Epoch 19/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 61.1826 - val_loss: 53.1398\n",
      "Epoch 20/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 60.3391 - val_loss: 52.9976\n",
      "Epoch 21/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 59.3731 - val_loss: 53.1204\n",
      "Epoch 22/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 59.0726 - val_loss: 56.1045\n",
      "Epoch 23/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 59.0511 - val_loss: 54.5783\n",
      "Epoch 24/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 57.6710 - val_loss: 53.1534\n",
      "Epoch 25/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 57.9489 - val_loss: 55.3791\n",
      "Epoch 26/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 57.4591 - val_loss: 55.4107\n",
      "Epoch 27/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 56.0894 - val_loss: 54.0519\n",
      "Epoch 28/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 56.6246 - val_loss: 52.7398\n",
      "Epoch 29/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 56.3202 - val_loss: 53.0652\n",
      "Epoch 30/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 56.1674 - val_loss: 53.2149\n",
      "Epoch 31/500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 55.4652 - val_loss: 54.6688\n",
      "Epoch 32/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 55.5274 - val_loss: 52.8539\n",
      "Epoch 33/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 54.9200 - val_loss: 52.7846\n",
      "Epoch 34/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 54.4207 - val_loss: 52.9806\n",
      "Epoch 35/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 54.4454 - val_loss: 53.0064\n",
      "Epoch 36/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 54.3330 - val_loss: 52.6017\n",
      "Epoch 37/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 54.1114 - val_loss: 52.7942\n",
      "Epoch 38/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 54.0277 - val_loss: 52.6897\n",
      "Epoch 39/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 53.7632 - val_loss: 53.1665\n",
      "Epoch 40/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 54.0999 - val_loss: 52.9269\n",
      "Epoch 41/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 53.5713 - val_loss: 52.5818\n",
      "Epoch 42/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 53.1418 - val_loss: 52.6297\n",
      "Epoch 43/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 53.4161 - val_loss: 52.6851\n",
      "Epoch 44/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 53.3588 - val_loss: 52.4544\n",
      "Epoch 45/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 53.2028 - val_loss: 52.6095\n",
      "Epoch 46/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 52.7341 - val_loss: 52.6148\n",
      "Epoch 47/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 52.8502 - val_loss: 52.5664\n",
      "Epoch 48/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 52.5273 - val_loss: 52.3763\n",
      "Epoch 49/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 52.6843 - val_loss: 52.4628\n",
      "Epoch 50/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 52.5199 - val_loss: 52.3501\n",
      "Epoch 51/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 52.1194 - val_loss: 52.3445\n",
      "Epoch 52/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 52.3981 - val_loss: 52.1891\n",
      "Epoch 53/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 51.7613 - val_loss: 52.4160\n",
      "Epoch 54/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 51.8745 - val_loss: 53.0644\n",
      "Epoch 55/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 51.8717 - val_loss: 52.2035\n",
      "Epoch 56/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 51.5899 - val_loss: 52.1748\n",
      "Epoch 57/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 51.4701 - val_loss: 52.1875\n",
      "Epoch 58/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 51.3719 - val_loss: 52.3415\n",
      "Epoch 59/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 51.0179 - val_loss: 52.0792\n",
      "Epoch 60/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 51.0734 - val_loss: 51.9723\n",
      "Epoch 61/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 50.8993 - val_loss: 52.0882\n",
      "Epoch 62/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 51.1013 - val_loss: 51.8208\n",
      "Epoch 63/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 50.8967 - val_loss: 52.4678\n",
      "Epoch 64/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 50.7320 - val_loss: 51.8816\n",
      "Epoch 65/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 50.5166 - val_loss: 51.8589\n",
      "Epoch 66/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 50.5897 - val_loss: 52.9701\n",
      "Epoch 67/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 50.9702 - val_loss: 53.0129\n",
      "Epoch 68/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 50.1562 - val_loss: 53.4089\n",
      "Epoch 69/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 50.1619 - val_loss: 52.0184\n",
      "Epoch 70/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 50.2928 - val_loss: 51.7201\n",
      "Epoch 71/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 50.0144 - val_loss: 51.7669\n",
      "Epoch 72/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 49.8012 - val_loss: 51.5140\n",
      "Epoch 73/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 49.4637 - val_loss: 51.4314\n",
      "Epoch 74/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 49.9515 - val_loss: 51.5904\n",
      "Epoch 75/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 49.8954 - val_loss: 51.4507\n",
      "Epoch 76/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 49.6528 - val_loss: 51.5024\n",
      "Epoch 77/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 49.2280 - val_loss: 51.0361\n",
      "Epoch 78/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 49.1579 - val_loss: 51.2863\n",
      "Epoch 79/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 49.2520 - val_loss: 51.2809\n",
      "Epoch 80/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 48.8693 - val_loss: 50.8582\n",
      "Epoch 81/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 48.9004 - val_loss: 51.5694\n",
      "Epoch 82/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 48.8826 - val_loss: 51.0735\n",
      "Epoch 83/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 48.8958 - val_loss: 50.4854\n",
      "Epoch 84/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 48.8503 - val_loss: 50.7878\n",
      "Epoch 85/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 48.8544 - val_loss: 51.2177\n",
      "Epoch 86/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 48.3563 - val_loss: 51.7074\n",
      "Epoch 87/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 48.4559 - val_loss: 50.5881\n",
      "Epoch 88/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 48.4277 - val_loss: 50.9904\n",
      "Epoch 89/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 48.1559 - val_loss: 51.0290\n",
      "Epoch 90/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 47.9861 - val_loss: 51.0178\n",
      "Epoch 91/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 48.3433 - val_loss: 49.5437\n",
      "Epoch 92/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 47.9985 - val_loss: 50.0926\n",
      "Epoch 93/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 47.9730 - val_loss: 49.0303\n",
      "Epoch 94/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 47.7642 - val_loss: 49.6295\n",
      "Epoch 95/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 47.6057 - val_loss: 50.6295\n",
      "Epoch 96/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 47.6668 - val_loss: 50.1436\n",
      "Epoch 97/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 48.3080 - val_loss: 49.6470\n",
      "Epoch 98/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 48.3498 - val_loss: 50.3313\n",
      "Epoch 99/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 47.4014 - val_loss: 49.7675\n",
      "Epoch 100/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 47.6782 - val_loss: 49.0402\n",
      "Epoch 101/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 47.4476 - val_loss: 50.0577\n",
      "Epoch 102/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 47.5053 - val_loss: 49.1387\n",
      "Epoch 103/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 47.2735 - val_loss: 50.2826\n",
      "Epoch 104/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 47.2829 - val_loss: 49.1740\n",
      "Epoch 105/500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 47.2939 - val_loss: 48.5161\n",
      "Epoch 106/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 47.0676 - val_loss: 49.2844\n",
      "Epoch 107/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 46.9886 - val_loss: 46.5724\n",
      "Epoch 108/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 46.9610 - val_loss: 45.9471\n",
      "Epoch 109/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 46.6597 - val_loss: 48.1290\n",
      "Epoch 110/500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 47.1317 - val_loss: 49.1300\n",
      "Epoch 111/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 46.5938 - val_loss: 45.7278\n",
      "Epoch 112/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 46.4285 - val_loss: 46.6869\n",
      "Epoch 113/500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 47.0375 - val_loss: 46.2223\n",
      "Epoch 114/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 46.2978 - val_loss: 44.8784\n",
      "Epoch 115/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 46.2436 - val_loss: 47.9549\n",
      "Epoch 116/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 46.3757 - val_loss: 47.0204\n",
      "Epoch 117/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 46.1923 - val_loss: 44.6379\n",
      "Epoch 118/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 45.9891 - val_loss: 43.2521\n",
      "Epoch 119/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 45.8388 - val_loss: 43.7435\n",
      "Epoch 120/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 46.5131 - val_loss: 44.3906\n",
      "Epoch 121/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 45.7596 - val_loss: 42.8050\n",
      "Epoch 122/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 45.7934 - val_loss: 41.5002\n",
      "Epoch 123/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 45.3315 - val_loss: 44.2266\n",
      "Epoch 124/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 45.6947 - val_loss: 42.4912\n",
      "Epoch 125/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 45.1012 - val_loss: 42.5524\n",
      "Epoch 126/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 45.3920 - val_loss: 41.5182\n",
      "Epoch 127/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 44.9004 - val_loss: 42.3698\n",
      "Epoch 128/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 45.0210 - val_loss: 41.3281\n",
      "Epoch 129/500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 44.6676 - val_loss: 39.2491\n",
      "Epoch 130/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 45.0631 - val_loss: 39.9550\n",
      "Epoch 131/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 44.5847 - val_loss: 45.7516\n",
      "Epoch 132/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 44.4046 - val_loss: 38.5730\n",
      "Epoch 133/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 44.2611 - val_loss: 45.3660\n",
      "Epoch 134/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 44.1853 - val_loss: 40.6187\n",
      "Epoch 135/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 43.9809 - val_loss: 39.8750\n",
      "Epoch 136/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 43.9035 - val_loss: 40.5574\n",
      "Epoch 137/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 44.3424 - val_loss: 39.3146\n",
      "Epoch 138/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 44.0354 - val_loss: 38.4578\n",
      "Epoch 139/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 43.6084 - val_loss: 40.1549\n",
      "Epoch 140/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 43.3011 - val_loss: 36.1108\n",
      "Epoch 141/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 43.0466 - val_loss: 39.8100\n",
      "Epoch 142/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 43.0699 - val_loss: 37.6067\n",
      "Epoch 143/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 42.8209 - val_loss: 39.8721\n",
      "Epoch 144/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 42.7578 - val_loss: 43.6865\n",
      "Epoch 145/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 42.8699 - val_loss: 44.5133\n",
      "Epoch 146/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 42.9511 - val_loss: 37.0445\n",
      "Epoch 147/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 42.5367 - val_loss: 37.4676\n",
      "Epoch 148/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 42.0035 - val_loss: 37.0456\n",
      "Epoch 149/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 42.0311 - val_loss: 40.0482\n",
      "Epoch 150/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 41.6932 - val_loss: 38.6919\n",
      "Epoch 151/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 41.3817 - val_loss: 38.8006\n",
      "Epoch 152/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 41.1747 - val_loss: 37.4646\n",
      "Epoch 153/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 40.9379 - val_loss: 36.7088\n",
      "Epoch 154/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 41.0116 - val_loss: 37.2290\n",
      "Epoch 155/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 40.4641 - val_loss: 38.5499\n",
      "Epoch 156/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 40.3657 - val_loss: 36.0992\n",
      "Epoch 157/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 40.6836 - val_loss: 40.0711\n",
      "Epoch 158/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 40.2429 - val_loss: 36.7754\n",
      "Epoch 159/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 39.5597 - val_loss: 35.1961\n",
      "Epoch 160/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 40.0777 - val_loss: 33.5614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 39.4590 - val_loss: 37.9722\n",
      "Epoch 162/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 39.2508 - val_loss: 36.2872\n",
      "Epoch 163/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 38.8724 - val_loss: 35.2424\n",
      "Epoch 164/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 38.5191 - val_loss: 34.0425\n",
      "Epoch 165/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 38.8792 - val_loss: 31.2496\n",
      "Epoch 166/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 39.0280 - val_loss: 41.6097\n",
      "Epoch 167/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 38.9054 - val_loss: 32.5330\n",
      "Epoch 168/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 38.0065 - val_loss: 31.1652\n",
      "Epoch 169/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 38.0605 - val_loss: 30.3782\n",
      "Epoch 170/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 37.6177 - val_loss: 32.8222\n",
      "Epoch 171/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 36.8437 - val_loss: 31.4102\n",
      "Epoch 172/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 37.1145 - val_loss: 32.2410\n",
      "Epoch 173/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 37.7973 - val_loss: 29.4498\n",
      "Epoch 174/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 36.7687 - val_loss: 31.1482\n",
      "Epoch 175/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 36.6846 - val_loss: 28.8898\n",
      "Epoch 176/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 35.7931 - val_loss: 31.9695\n",
      "Epoch 177/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 35.1229 - val_loss: 27.9316\n",
      "Epoch 178/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 35.3450 - val_loss: 29.9363\n",
      "Epoch 179/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 35.3568 - val_loss: 27.9707\n",
      "Epoch 180/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 34.4610 - val_loss: 29.6460\n",
      "Epoch 181/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 35.1642 - val_loss: 31.8373\n",
      "Epoch 182/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 34.8410 - val_loss: 32.6374\n",
      "Epoch 183/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 34.4727 - val_loss: 31.1771\n",
      "Epoch 184/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 34.2037 - val_loss: 29.6564\n",
      "Epoch 185/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 33.3894 - val_loss: 29.8309\n",
      "Epoch 186/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 32.5909 - val_loss: 27.5931\n",
      "Epoch 187/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 32.6108 - val_loss: 25.1665\n",
      "Epoch 188/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 32.5827 - val_loss: 27.4509\n",
      "Epoch 189/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 31.9022 - val_loss: 25.3641\n",
      "Epoch 190/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 31.6167 - val_loss: 23.9900\n",
      "Epoch 191/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 30.9523 - val_loss: 23.4265\n",
      "Epoch 192/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 31.4170 - val_loss: 21.3333\n",
      "Epoch 193/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 30.8192 - val_loss: 22.3275\n",
      "Epoch 194/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 30.8141 - val_loss: 24.0786\n",
      "Epoch 195/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 29.9778 - val_loss: 20.2837\n",
      "Epoch 196/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 29.5981 - val_loss: 21.5579\n",
      "Epoch 197/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 30.0742 - val_loss: 23.3535\n",
      "Epoch 198/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 28.6031 - val_loss: 21.7755\n",
      "Epoch 199/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 28.3382 - val_loss: 20.6194\n",
      "Epoch 200/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 28.3655 - val_loss: 17.3236\n",
      "Epoch 201/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 28.0754 - val_loss: 19.3240\n",
      "Epoch 202/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 26.9122 - val_loss: 17.4086\n",
      "Epoch 203/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 27.1491 - val_loss: 18.2192\n",
      "Epoch 204/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 27.7292 - val_loss: 18.1183\n",
      "Epoch 205/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 27.8107 - val_loss: 23.0522\n",
      "Epoch 206/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 27.0916 - val_loss: 15.6916\n",
      "Epoch 207/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 26.0352 - val_loss: 17.2095\n",
      "Epoch 208/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 25.3875 - val_loss: 14.6424\n",
      "Epoch 209/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 25.4857 - val_loss: 13.1814\n",
      "Epoch 210/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 25.0566 - val_loss: 11.6909\n",
      "Epoch 211/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 25.5124 - val_loss: 12.2649\n",
      "Epoch 212/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 24.8456 - val_loss: 13.5251\n",
      "Epoch 213/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 24.2836 - val_loss: 18.0251\n",
      "Epoch 214/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 23.9218 - val_loss: 14.6106\n",
      "Epoch 215/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 23.5991 - val_loss: 19.0468\n",
      "Epoch 216/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 23.0630 - val_loss: 14.0514\n",
      "Epoch 217/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 23.5965 - val_loss: 10.6093\n",
      "Epoch 218/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 23.4764 - val_loss: 11.2580\n",
      "Epoch 219/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 22.2743 - val_loss: 11.9786\n",
      "Epoch 220/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 24.3604 - val_loss: 11.8842\n",
      "Epoch 221/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 21.7978 - val_loss: 11.4345\n",
      "Epoch 222/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 21.7307 - val_loss: 8.6280\n",
      "Epoch 223/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 21.8126 - val_loss: 10.0406\n",
      "Epoch 224/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 20.8437 - val_loss: 9.8547\n",
      "Epoch 225/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 20.8493 - val_loss: 10.2383\n",
      "Epoch 226/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 20.7020 - val_loss: 12.1297\n",
      "Epoch 227/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 20.1449 - val_loss: 9.3622\n",
      "Epoch 228/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 19.5848 - val_loss: 8.7549\n",
      "Epoch 229/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 19.8597 - val_loss: 13.7312\n",
      "Epoch 230/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 19.7493 - val_loss: 9.4262\n",
      "Epoch 231/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 19.0912 - val_loss: 8.5527\n",
      "Epoch 232/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 19.8327 - val_loss: 7.9170\n",
      "Epoch 233/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 18.6830 - val_loss: 13.9598\n",
      "Epoch 234/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 18.5607 - val_loss: 8.2997\n",
      "Epoch 235/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 19.7804 - val_loss: 16.8403\n",
      "Epoch 236/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 18.7944 - val_loss: 7.6811\n",
      "Epoch 237/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 18.2515 - val_loss: 8.2099\n",
      "Epoch 238/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 17.4052 - val_loss: 11.9447\n",
      "Epoch 239/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 16.8381 - val_loss: 8.7966\n",
      "Epoch 240/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 17.5847 - val_loss: 9.7644\n",
      "Epoch 241/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 16.3046 - val_loss: 9.2315\n",
      "Epoch 242/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 16.0590 - val_loss: 8.1438\n",
      "Epoch 243/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 16.8063 - val_loss: 8.7300\n",
      "Epoch 244/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 15.9698 - val_loss: 10.3455\n",
      "Epoch 245/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 15.7384 - val_loss: 11.5448\n",
      "Epoch 246/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 16.0545 - val_loss: 9.4654\n",
      "Epoch 247/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 16.0642 - val_loss: 8.4622\n",
      "Epoch 248/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 14.1241 - val_loss: 7.2201\n",
      "Epoch 249/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 15.2872 - val_loss: 12.3155\n",
      "Epoch 250/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 15.0770 - val_loss: 7.9265\n",
      "Epoch 251/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 14.1429 - val_loss: 7.5539\n",
      "Epoch 252/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 15.2158 - val_loss: 11.1991\n",
      "Epoch 253/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 14.9282 - val_loss: 10.4931\n",
      "Epoch 254/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.8029 - val_loss: 7.4478\n",
      "Epoch 255/500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 13.5117 - val_loss: 7.5779\n",
      "Epoch 256/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 14.6272 - val_loss: 6.9329\n",
      "Epoch 257/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 13.5636 - val_loss: 8.5352\n",
      "Epoch 258/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 13.1319 - val_loss: 8.4036\n",
      "Epoch 259/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 13.8251 - val_loss: 9.6250\n",
      "Epoch 260/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 13.0307 - val_loss: 11.1343\n",
      "Epoch 261/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.7371 - val_loss: 9.8835\n",
      "Epoch 262/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.8044 - val_loss: 9.6374\n",
      "Epoch 263/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 13.9404 - val_loss: 6.9609\n",
      "Epoch 264/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 13.7284 - val_loss: 11.8432\n",
      "Epoch 265/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 14.1634 - val_loss: 7.3293\n",
      "Epoch 266/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 14.2640 - val_loss: 13.0455\n",
      "Epoch 267/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 13.6234 - val_loss: 11.1147\n",
      "Epoch 268/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.2594 - val_loss: 10.4566\n",
      "Epoch 269/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.1512 - val_loss: 9.0240\n",
      "Epoch 270/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.3333 - val_loss: 6.8501\n",
      "Epoch 271/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.9806 - val_loss: 7.0720\n",
      "Epoch 272/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.5853 - val_loss: 9.1250\n",
      "Epoch 273/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.9456 - val_loss: 7.8566\n",
      "Epoch 274/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.9708 - val_loss: 10.1937\n",
      "Epoch 275/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.1435 - val_loss: 8.5189\n",
      "Epoch 276/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.7720 - val_loss: 7.0538\n",
      "Epoch 277/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.0269 - val_loss: 7.9603\n",
      "Epoch 278/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.9751 - val_loss: 10.0328\n",
      "Epoch 279/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 13.1610 - val_loss: 7.0273\n",
      "Epoch 280/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 13.3285 - val_loss: 6.8560\n",
      "Epoch 281/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.9693 - val_loss: 8.8311\n",
      "Epoch 282/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 13.1081 - val_loss: 8.2188\n",
      "Epoch 283/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 13.2660 - val_loss: 7.1223\n",
      "Epoch 284/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 13.8400 - val_loss: 6.8966\n",
      "Epoch 285/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.1034 - val_loss: 7.9174\n",
      "Epoch 286/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.5087 - val_loss: 8.9218\n",
      "Epoch 287/500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 12.0768 - val_loss: 7.2927\n",
      "Epoch 288/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.8751 - val_loss: 10.3610\n",
      "Epoch 289/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.6940 - val_loss: 6.9354\n",
      "Epoch 290/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.7928 - val_loss: 8.5419\n",
      "Epoch 291/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.9844 - val_loss: 9.3763\n",
      "Epoch 292/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 13.1107 - val_loss: 7.7811\n",
      "Epoch 293/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.4617 - val_loss: 7.1775\n",
      "Epoch 294/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.4157 - val_loss: 7.2600\n",
      "Epoch 295/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 13.3198 - val_loss: 7.0527\n",
      "Epoch 296/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.8133 - val_loss: 8.2715\n",
      "Epoch 297/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.0428 - val_loss: 6.5868\n",
      "Epoch 298/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.5653 - val_loss: 14.0118\n",
      "Epoch 299/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.7705 - val_loss: 8.3482\n",
      "Epoch 300/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 13.0245 - val_loss: 7.1181\n",
      "Epoch 301/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.8474 - val_loss: 7.1236\n",
      "Epoch 302/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.7595 - val_loss: 10.3676\n",
      "Epoch 303/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.7683 - val_loss: 11.5536\n",
      "Epoch 304/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 13.4210 - val_loss: 7.2903\n",
      "Epoch 305/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.5148 - val_loss: 7.3430\n",
      "Epoch 306/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 14.4724 - val_loss: 15.8314\n",
      "Epoch 307/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.4134 - val_loss: 10.5980\n",
      "Epoch 308/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.1572 - val_loss: 6.9606\n",
      "Epoch 309/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.6502 - val_loss: 8.7009\n",
      "Epoch 310/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.5608 - val_loss: 9.3317\n",
      "Epoch 311/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.4544 - val_loss: 15.0433\n",
      "Epoch 312/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.2898 - val_loss: 8.5781\n",
      "Epoch 313/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.1850 - val_loss: 9.7597\n",
      "Epoch 314/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.5484 - val_loss: 7.0190\n",
      "Epoch 315/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.3440 - val_loss: 7.8358\n",
      "Epoch 316/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 13.0584 - val_loss: 6.9770\n",
      "Epoch 317/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.1616 - val_loss: 8.9068\n",
      "Epoch 318/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.7174 - val_loss: 8.5528\n",
      "Epoch 319/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 13.0441 - val_loss: 6.6726\n",
      "Epoch 320/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 4ms/step - loss: 12.5088 - val_loss: 6.7919\n",
      "Epoch 321/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.8472 - val_loss: 9.2962\n",
      "Epoch 322/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.3660 - val_loss: 6.8030\n",
      "Epoch 323/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.6909 - val_loss: 7.6236\n",
      "Epoch 324/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.2402 - val_loss: 7.6986\n",
      "Epoch 325/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.1223 - val_loss: 6.8639\n",
      "Epoch 326/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.8420 - val_loss: 9.0039\n",
      "Epoch 327/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.8036 - val_loss: 7.2418\n",
      "Epoch 328/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.9381 - val_loss: 10.0494\n",
      "Epoch 329/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 13.4443 - val_loss: 11.3782\n",
      "Epoch 330/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.9949 - val_loss: 8.7332\n",
      "Epoch 331/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.7096 - val_loss: 7.0455\n",
      "Epoch 332/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.2543 - val_loss: 7.8550\n",
      "Epoch 333/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.9747 - val_loss: 13.2680\n",
      "Epoch 334/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.9345 - val_loss: 8.9705\n",
      "Epoch 335/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.0078 - val_loss: 7.2947\n",
      "Epoch 336/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.6500 - val_loss: 6.8743\n",
      "Epoch 337/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.5955 - val_loss: 6.9993\n",
      "Epoch 338/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.1192 - val_loss: 7.0195\n",
      "Epoch 339/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 13.2052 - val_loss: 7.9307\n",
      "Epoch 340/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.0674 - val_loss: 7.0478\n",
      "Epoch 341/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.4038 - val_loss: 8.7235\n",
      "Epoch 342/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.7838 - val_loss: 7.6507\n",
      "Epoch 343/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.9444 - val_loss: 6.8645\n",
      "Epoch 344/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.0767 - val_loss: 7.4677\n",
      "Epoch 345/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.1550 - val_loss: 9.7031\n",
      "Epoch 346/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.9675 - val_loss: 7.7149\n",
      "Epoch 347/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.7756 - val_loss: 12.8040\n",
      "Epoch 348/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 13.0126 - val_loss: 7.0295\n",
      "Epoch 349/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.1742 - val_loss: 7.8851\n",
      "Epoch 350/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.8645 - val_loss: 9.8296\n",
      "Epoch 351/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.0774 - val_loss: 7.7803\n",
      "Epoch 352/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.1957 - val_loss: 7.4797\n",
      "Epoch 353/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.7020 - val_loss: 6.9011\n",
      "Epoch 354/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.1175 - val_loss: 9.5572\n",
      "Epoch 355/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.7301 - val_loss: 7.3598\n",
      "Epoch 356/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.6327 - val_loss: 9.4300\n",
      "Epoch 357/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.4516 - val_loss: 9.1773\n",
      "Epoch 358/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 13.0464 - val_loss: 14.9046\n",
      "Epoch 359/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.8572 - val_loss: 7.6412\n",
      "Epoch 360/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.4587 - val_loss: 7.9001\n",
      "Epoch 361/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.0730 - val_loss: 10.8968\n",
      "Epoch 362/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.0726 - val_loss: 9.6842\n",
      "Epoch 363/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 13.2058 - val_loss: 6.7930\n",
      "Epoch 364/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.9669 - val_loss: 7.6291\n",
      "Epoch 365/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.4836 - val_loss: 7.0660\n",
      "Epoch 366/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.5462 - val_loss: 8.8109\n",
      "Epoch 367/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.3306 - val_loss: 11.2224\n",
      "Epoch 368/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 13.7788 - val_loss: 9.3054\n",
      "Epoch 369/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.0438 - val_loss: 13.1005\n",
      "Epoch 370/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.8381 - val_loss: 8.2066\n",
      "Epoch 371/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.6867 - val_loss: 6.8922\n",
      "Epoch 372/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.3619 - val_loss: 7.9748\n",
      "Epoch 373/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.8078 - val_loss: 7.2970\n",
      "Epoch 374/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 10.7888 - val_loss: 8.0026\n",
      "Epoch 375/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.1467 - val_loss: 8.4830\n",
      "Epoch 376/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.4639 - val_loss: 8.3610\n",
      "Epoch 377/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.0489 - val_loss: 8.6544\n",
      "Epoch 378/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.5315 - val_loss: 6.6483\n",
      "Epoch 379/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.6857 - val_loss: 8.7698\n",
      "Epoch 380/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.7336 - val_loss: 7.0634\n",
      "Epoch 381/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.6060 - val_loss: 8.0445\n",
      "Epoch 382/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.5320 - val_loss: 9.3950\n",
      "Epoch 383/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.0118 - val_loss: 8.1051\n",
      "Epoch 384/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.1878 - val_loss: 9.0172\n",
      "Epoch 385/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.3986 - val_loss: 7.7368\n",
      "Epoch 386/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.1764 - val_loss: 9.8740\n",
      "Epoch 387/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.5102 - val_loss: 14.0599\n",
      "Epoch 388/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.8587 - val_loss: 7.9904\n",
      "Epoch 389/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.0072 - val_loss: 9.8108\n",
      "Epoch 390/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.5012 - val_loss: 8.0341\n",
      "Epoch 391/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.8065 - val_loss: 7.0537\n",
      "Epoch 392/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.7313 - val_loss: 7.4918\n",
      "Epoch 393/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.4466 - val_loss: 7.5028\n",
      "Epoch 394/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.7340 - val_loss: 6.5771\n",
      "Epoch 395/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.3127 - val_loss: 6.6228\n",
      "Epoch 396/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.2563 - val_loss: 7.6403\n",
      "Epoch 397/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.3628 - val_loss: 9.4362\n",
      "Epoch 398/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.7805 - val_loss: 7.9130\n",
      "Epoch 399/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.3194 - val_loss: 9.2527\n",
      "Epoch 400/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.6900 - val_loss: 7.1920\n",
      "Epoch 401/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 13.2285 - val_loss: 8.3869\n",
      "Epoch 402/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.6433 - val_loss: 6.9835\n",
      "Epoch 403/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.7114 - val_loss: 11.2138\n",
      "Epoch 404/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.6380 - val_loss: 7.5645\n",
      "Epoch 405/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.5260 - val_loss: 11.8285\n",
      "Epoch 406/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.8680 - val_loss: 10.7483\n",
      "Epoch 407/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.6685 - val_loss: 8.2065\n",
      "Epoch 408/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.7013 - val_loss: 6.7977\n",
      "Epoch 409/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.2755 - val_loss: 9.3384\n",
      "Epoch 410/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.3597 - val_loss: 7.3912\n",
      "Epoch 411/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.2634 - val_loss: 10.4255\n",
      "Epoch 412/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 10.9687 - val_loss: 7.1399\n",
      "Epoch 413/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.3244 - val_loss: 7.9913\n",
      "Epoch 414/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.4627 - val_loss: 10.7383\n",
      "Epoch 415/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.1936 - val_loss: 8.5975\n",
      "Epoch 416/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.4705 - val_loss: 6.7113\n",
      "Epoch 417/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.2385 - val_loss: 11.8012\n",
      "Epoch 418/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.6848 - val_loss: 11.8840\n",
      "Epoch 419/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.7726 - val_loss: 7.9123\n",
      "Epoch 420/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.4258 - val_loss: 8.0287\n",
      "Epoch 421/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.1296 - val_loss: 9.6203\n",
      "Epoch 422/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.2684 - val_loss: 6.5220\n",
      "Epoch 423/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 10.6597 - val_loss: 8.6853\n",
      "Epoch 424/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.3568 - val_loss: 7.0455\n",
      "Epoch 425/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.2409 - val_loss: 6.7005\n",
      "Epoch 426/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.1506 - val_loss: 7.7342\n",
      "Epoch 427/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.8742 - val_loss: 7.7093\n",
      "Epoch 428/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 10.9514 - val_loss: 7.2119\n",
      "Epoch 429/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.5507 - val_loss: 7.9289\n",
      "Epoch 430/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.3407 - val_loss: 7.3597\n",
      "Epoch 431/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.2178 - val_loss: 7.9884\n",
      "Epoch 432/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.3251 - val_loss: 6.8941\n",
      "Epoch 433/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.1638 - val_loss: 7.3649\n",
      "Epoch 434/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 10.9022 - val_loss: 6.8748\n",
      "Epoch 435/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.4250 - val_loss: 7.8908\n",
      "Epoch 436/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.9121 - val_loss: 8.1762\n",
      "Epoch 437/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 10.9935 - val_loss: 8.0111\n",
      "Epoch 438/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.1944 - val_loss: 9.8854\n",
      "Epoch 439/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.7503 - val_loss: 6.6459\n",
      "Epoch 440/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.1823 - val_loss: 7.5370\n",
      "Epoch 441/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.4170 - val_loss: 9.2693\n",
      "Epoch 442/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.3403 - val_loss: 8.1309\n",
      "Epoch 443/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.3263 - val_loss: 7.0663\n",
      "Epoch 444/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.6767 - val_loss: 7.9716\n",
      "Epoch 445/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.5533 - val_loss: 7.1126\n",
      "Epoch 446/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.6125 - val_loss: 6.8819\n",
      "Epoch 447/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.2264 - val_loss: 8.0754\n",
      "Epoch 448/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.8308 - val_loss: 7.0261\n",
      "Epoch 449/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.2979 - val_loss: 7.7475\n",
      "Epoch 450/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.7934 - val_loss: 8.3193\n",
      "Epoch 451/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.4057 - val_loss: 9.2603\n",
      "Epoch 452/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.4908 - val_loss: 8.9362\n",
      "Epoch 453/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.6077 - val_loss: 7.8041\n",
      "Epoch 454/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.6950 - val_loss: 7.5425\n",
      "Epoch 455/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.4865 - val_loss: 8.0279\n",
      "Epoch 456/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.0159 - val_loss: 7.5690\n",
      "Epoch 457/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.4957 - val_loss: 6.6665\n",
      "Epoch 458/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.5484 - val_loss: 7.1506\n",
      "Epoch 459/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.6593 - val_loss: 12.1406\n",
      "Epoch 460/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.4518 - val_loss: 10.2498\n",
      "Epoch 461/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.9368 - val_loss: 9.8571\n",
      "Epoch 462/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.4207 - val_loss: 11.4286\n",
      "Epoch 463/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.6535 - val_loss: 6.8520\n",
      "Epoch 464/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.4505 - val_loss: 7.4064\n",
      "Epoch 465/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.2981 - val_loss: 9.8532\n",
      "Epoch 466/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.2075 - val_loss: 13.6091\n",
      "Epoch 467/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.7786 - val_loss: 8.1483\n",
      "Epoch 468/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.3612 - val_loss: 7.7023\n",
      "Epoch 469/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.3556 - val_loss: 7.7042\n",
      "Epoch 470/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.7147 - val_loss: 6.5509\n",
      "Epoch 471/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.5892 - val_loss: 10.8670\n",
      "Epoch 472/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.0336 - val_loss: 8.0182\n",
      "Epoch 473/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.1727 - val_loss: 10.1204\n",
      "Epoch 474/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.6622 - val_loss: 7.2258\n",
      "Epoch 475/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.9121 - val_loss: 6.8847\n",
      "Epoch 476/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.3833 - val_loss: 7.0321\n",
      "Epoch 477/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.3655 - val_loss: 7.3367\n",
      "Epoch 478/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 12.1511 - val_loss: 8.1123\n",
      "Epoch 479/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.4047 - val_loss: 9.7183\n",
      "Epoch 480/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 3ms/step - loss: 11.5339 - val_loss: 7.1330\n",
      "Epoch 481/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.6248 - val_loss: 7.3307\n",
      "Epoch 482/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.0959 - val_loss: 7.0405\n",
      "Epoch 483/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.3660 - val_loss: 10.2431\n",
      "Epoch 484/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.7885 - val_loss: 9.5090\n",
      "Epoch 485/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.7970 - val_loss: 6.9992\n",
      "Epoch 486/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.9613 - val_loss: 8.1051\n",
      "Epoch 487/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.5248 - val_loss: 8.3801\n",
      "Epoch 488/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.4931 - val_loss: 6.7323\n",
      "Epoch 489/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.2339 - val_loss: 6.9289\n",
      "Epoch 490/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.6616 - val_loss: 7.9943\n",
      "Epoch 491/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.7387 - val_loss: 7.8687\n",
      "Epoch 492/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.7821 - val_loss: 9.2766\n",
      "Epoch 493/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 13.8039 - val_loss: 9.8669\n",
      "Epoch 494/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.3087 - val_loss: 7.7440\n",
      "Epoch 495/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.1924 - val_loss: 8.5086\n",
      "Epoch 496/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 11.9845 - val_loss: 8.2201\n",
      "Epoch 497/500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 11.1694 - val_loss: 7.4511\n",
      "Epoch 498/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 12.5612 - val_loss: 9.5312\n",
      "Epoch 499/500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 12.7658 - val_loss: 8.7998\n",
      "Epoch 500/500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 11.5190 - val_loss: 12.8326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11760"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def create_model(loss_fn,X_tr):\n",
    "    inps = Input(shape=(X_tr.shape[1],))\n",
    "    x = Dense(512, activation='relu')(inps)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(2, activation='relu')(x)\n",
    "    model = Model(inputs=inps, outputs=x)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=[loss_fn]\n",
    "    )\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "import datetime\n",
    "startdate = \"20191015\"\n",
    "features = [x for x in df_feature.columns if x not in ['report_date','amount','amount2']]\n",
    "\n",
    "num_labels = 2  # two task\n",
    "\n",
    "\n",
    "target = ['amount2','amount']  \n",
    "X_train = df_feature[df_feature['report_date']<startdate][features]\n",
    "y_train = df_feature[df_feature['report_date']<startdate][target].values\n",
    "X_valid = df_feature[df_feature['report_date']>=startdate][features]\n",
    "y_valid = df_feature[df_feature['report_date']>=startdate][target].values\n",
    "\n",
    "\n",
    "\n",
    "model = create_model('mape',X_train)\n",
    "model.fit(\n",
    "    X_train, y_train, epochs=500, batch_size=25, validation_data=(X_valid, y_valid), verbose=True)\n",
    "del model\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MT-GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          amount2    amount\n",
      "amount2  1.000000  0.094927\n",
      "amount   0.094927  1.000000\n",
      "(942, 22) (31, 22)\n",
      "(942, 2) (31, 2)\n",
      "[10]\ttraining's rmse: 5181.82\tvalid_1's rmse: 4431.63\n",
      "[20]\ttraining's rmse: 3845.48\tvalid_1's rmse: 3232.4\n",
      "[30]\ttraining's rmse: 2865.44\tvalid_1's rmse: 2355.34\n",
      "[40]\ttraining's rmse: 2148.61\tvalid_1's rmse: 1715.1\n",
      "[50]\ttraining's rmse: 1627.61\tvalid_1's rmse: 1245.1\n",
      "[60]\ttraining's rmse: 1252.37\tvalid_1's rmse: 903.056\n",
      "[70]\ttraining's rmse: 984.776\tvalid_1's rmse: 655.665\n",
      "[80]\ttraining's rmse: 797.835\tvalid_1's rmse: 484.398\n",
      "[90]\ttraining's rmse: 667.766\tvalid_1's rmse: 381.847\n",
      "[100]\ttraining's rmse: 578.947\tvalid_1's rmse: 331.724\n",
      "[110]\ttraining's rmse: 521.247\tvalid_1's rmse: 310.186\n",
      "[120]\ttraining's rmse: 482.569\tvalid_1's rmse: 308.206\n",
      "[130]\ttraining's rmse: 455.571\tvalid_1's rmse: 316.263\n",
      "[140]\ttraining's rmse: 436.132\tvalid_1's rmse: 327.626\n",
      "[150]\ttraining's rmse: 420.056\tvalid_1's rmse: 336.824\n",
      "[160]\ttraining's rmse: 407.328\tvalid_1's rmse: 344.532\n",
      "[170]\ttraining's rmse: 396.854\tvalid_1's rmse: 351.366\n",
      "[180]\ttraining's rmse: 390.015\tvalid_1's rmse: 356.389\n",
      "[190]\ttraining's rmse: 383.396\tvalid_1's rmse: 360.11\n",
      "[200]\ttraining's rmse: 377.142\tvalid_1's rmse: 363.75\n"
     ]
    }
   ],
   "source": [
    "startdate = \"20191015\"\n",
    "features = [x for x in df_feature.columns if x not in ['report_date','amount','amount2']]\n",
    "\n",
    "num_labels = 2  # åŒlabel\n",
    "\n",
    "def self_metric(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    labels2 = labels.reshape((num_labels,-1)).transpose()[:,1]\n",
    "    preds2 = preds.reshape((num_labels,-1)).transpose()[:,1]\n",
    "    score = np.mean((labels2-preds2) ** 2)**0.5\n",
    "    return 'rmse', score, False\n",
    "\n",
    "def mymse2(preds, train_data, ep = 0):\n",
    "    labels = train_data.get_label()\n",
    "    labels2 = labels.reshape((num_labels,-1)).transpose()    \n",
    "    preds2 = preds.reshape((num_labels,-1)).transpose()\n",
    "    grad2 = (preds2 - labels2)                               \n",
    "    grad = grad2 * np.array([1.5,0.001])                     \n",
    "    grad = np.sum(grad,axis = 1)\n",
    "    grad2 = grad2.transpose().reshape((-1))                  \n",
    "    hess = grad * 0. + 1\n",
    "    hess2 = grad2 * 0. + 1\n",
    "    return grad, hess, grad2, hess2                         \n",
    "\n",
    "\n",
    "\n",
    "param = {\n",
    "    'num_leaves':48, \n",
    "    'max_depth':6,\n",
    "    'learning_rate':.03,\n",
    "    'max_bin':200,\n",
    "    'lambda_l1':0.1,\n",
    "    'lambda_l2':0.2,\n",
    "    'verbose': 5,\n",
    "\n",
    "    # multitask\n",
    "    'objective':'custom',           \n",
    "    'num_labels':num_labels, \n",
    "    'tree_learner': 'serial2',\n",
    "    'num_threads':4}    \n",
    "\n",
    "\n",
    "target = ['amount2','amount']   # ç¬¬ä¸€ä¸ªlabelä¸ºè¾ƒå‰ä¸€å¤©å¢žé•¿ç™¾åˆ†æ¯”ï¼Œç¬¬äºŒä¸ªlabelä¸ºç»å¯¹å€¼\n",
    "X_train = df_feature[df_feature['report_date']<startdate][features]\n",
    "y_train = df_feature[df_feature['report_date']<startdate][target].values\n",
    "X_valid = df_feature[df_feature['report_date']>=startdate][features]\n",
    "y_valid = df_feature[df_feature['report_date']>=startdate][target].values\n",
    "\n",
    "print(df_feature[target].corr())\n",
    "print(X_train.shape,X_valid.shape)\n",
    "print(y_train.shape,y_valid.shape)\n",
    "evals_result_mt = {}\n",
    "train_data=lgb.Dataset(X_train,label=y_train)\n",
    "validation_data=lgb.Dataset(X_valid,label=y_valid)\n",
    "clf=lgb.train(param,train_data,verbose_eval=10,\n",
    "                fobj = mymse2,feval = self_metric,\n",
    "                num_boost_round=200,valid_sets=[train_data,validation_data],\n",
    "            evals_result=evals_result_mt)\n",
    "clf.set_num_labels(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(942, 22) (31, 22)\n",
      "[5]\ttraining's rmse: 2238.1\tvalid_1's rmse: 785.518\n",
      "[10]\ttraining's rmse: 1949.35\tvalid_1's rmse: 704.933\n",
      "[15]\ttraining's rmse: 1704.06\tvalid_1's rmse: 644.393\n",
      "[20]\ttraining's rmse: 1495.61\tvalid_1's rmse: 594.076\n",
      "[25]\ttraining's rmse: 1319.01\tvalid_1's rmse: 558.21\n",
      "[30]\ttraining's rmse: 1170.21\tvalid_1's rmse: 531.221\n",
      "[35]\ttraining's rmse: 1045.07\tvalid_1's rmse: 502.221\n",
      "[40]\ttraining's rmse: 940.485\tvalid_1's rmse: 481.079\n",
      "[45]\ttraining's rmse: 852.554\tvalid_1's rmse: 468.983\n",
      "[50]\ttraining's rmse: 780.007\tvalid_1's rmse: 453.633\n",
      "[55]\ttraining's rmse: 720.151\tvalid_1's rmse: 440.374\n",
      "[60]\ttraining's rmse: 670.974\tvalid_1's rmse: 438.151\n",
      "[65]\ttraining's rmse: 630.445\tvalid_1's rmse: 442.611\n",
      "[70]\ttraining's rmse: 597.238\tvalid_1's rmse: 439.56\n",
      "[75]\ttraining's rmse: 568.735\tvalid_1's rmse: 436.23\n",
      "[80]\ttraining's rmse: 545.337\tvalid_1's rmse: 429.793\n",
      "[85]\ttraining's rmse: 525.435\tvalid_1's rmse: 428.034\n",
      "[90]\ttraining's rmse: 508.725\tvalid_1's rmse: 423.824\n",
      "[95]\ttraining's rmse: 492.902\tvalid_1's rmse: 419.983\n",
      "[100]\ttraining's rmse: 479.058\tvalid_1's rmse: 418.006\n",
      "[105]\ttraining's rmse: 467.491\tvalid_1's rmse: 413.17\n",
      "[110]\ttraining's rmse: 457.619\tvalid_1's rmse: 412.763\n",
      "[115]\ttraining's rmse: 448.654\tvalid_1's rmse: 411.774\n",
      "[120]\ttraining's rmse: 441.34\tvalid_1's rmse: 408.969\n",
      "[125]\ttraining's rmse: 434.146\tvalid_1's rmse: 406.623\n",
      "[130]\ttraining's rmse: 427.432\tvalid_1's rmse: 406.747\n",
      "[135]\ttraining's rmse: 420.538\tvalid_1's rmse: 407.94\n",
      "[140]\ttraining's rmse: 414.586\tvalid_1's rmse: 405.918\n",
      "[145]\ttraining's rmse: 408.791\tvalid_1's rmse: 403.475\n",
      "[150]\ttraining's rmse: 403.43\tvalid_1's rmse: 403.472\n",
      "[155]\ttraining's rmse: 398.17\tvalid_1's rmse: 401.586\n",
      "[160]\ttraining's rmse: 393.716\tvalid_1's rmse: 400.318\n",
      "[165]\ttraining's rmse: 389.421\tvalid_1's rmse: 403.229\n",
      "[170]\ttraining's rmse: 384.881\tvalid_1's rmse: 403.214\n",
      "[175]\ttraining's rmse: 380.459\tvalid_1's rmse: 402.073\n",
      "[180]\ttraining's rmse: 376.721\tvalid_1's rmse: 404.971\n",
      "[185]\ttraining's rmse: 373.216\tvalid_1's rmse: 405.132\n",
      "[190]\ttraining's rmse: 369.846\tvalid_1's rmse: 404.165\n",
      "[195]\ttraining's rmse: 367.113\tvalid_1's rmse: 403.488\n",
      "[200]\ttraining's rmse: 364.428\tvalid_1's rmse: 403.338\n"
     ]
    }
   ],
   "source": [
    "startdate = \"20191015\"\n",
    "features = [x for x in df_feature.columns if x not in ['report_date','amount','amount2']]\n",
    "param = {\n",
    "    'num_leaves':48, \n",
    "    'objective':'regression',\n",
    "    'max_depth':6,\n",
    "    'learning_rate':.03,\n",
    "    'max_bin':200,\n",
    "    'lambda_l1':0.1,\n",
    "    'lambda_l2':0.2,\n",
    "    'verboses':10,\n",
    "    'metrics':\"rmse\",\n",
    "    'num_threads':4}\n",
    "\n",
    "def self_metric(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    score = np.mean(np.abs(labels-preds)/(np.abs(labels)+0.0001))\n",
    "    return 'self_metric', score, False\n",
    "\n",
    "target = 'amount'\n",
    "X_train = df_feature[df_feature['report_date']<startdate][features]\n",
    "y_train = df_feature[df_feature['report_date']<startdate][target]\n",
    "X_valid = df_feature[df_feature['report_date']>=startdate][features]\n",
    "y_valid = df_feature[df_feature['report_date']>=startdate][target]\n",
    "\n",
    "print(X_train.shape,X_valid.shape)\n",
    "evals_result_singlelgb = {}\n",
    "train_data=lgb.Dataset(X_train,label=y_train)\n",
    "validation_data=lgb.Dataset(X_valid,label=y_valid)\n",
    "clf=lgb.train(param,train_data,verbose_eval=5,\n",
    "                num_boost_round=200,valid_sets=[train_data,validation_data],evals_result = evals_result_singlelgb)\n",
    "clf.set_num_labels(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdd3hUVfrA8e87yaT3kIQklCRIgkCkd5CiIiBFsYIFy4q6uLr+XAsWEFZ3XXUtrF1ERGwoooBIUyIqIIYQIPTQk0AKEEIC6ef3x53gAIFMkplMyvk8z30yc+aec9+ZwLy55557jiil0DRN0zR7MDk7AE3TNK3x0ElF0zRNsxudVDRN0zS70UlF0zRNsxudVDRN0zS70UlF0zRNsxudVLRGTUSeEpGZdmjnORGZ6+w4HEVEfhCRCfbeV2t6RN+notU1EdkPRAARSqkcq/KNQGcgWim1v4o2BgFzlVItHBfpWcd7DrhEKXVbXRyvOkREAW2VUqnOjkXT9JmK5iz7gHEVT0QkHvCy5wFExNWe7dWUs+Nw9vG1pkUnFc1ZPgHusHo+AZhjvYOIuIvIKyJyUEQyReRdEfEUEW/gByBCRPItW4Sli+prEZkrInnAned2W4lIfxFZIyK5InJIRO6sLDgRiRaRn0XkpIisAJpZvTZIRNLO2X+/iFxpeXzROEQkSkSUiEywvLccEXnaqi1PEflYRI6LyHYRefzc41ntu9rycJPlc7i5Ij4ReUJEjgAfiUigiCwWkWxLu4tFpIVVOwki8hfL4ztF5FfLZ39cRPaJyPAa7hstIqstn+NKEXmrNt2IWv2nk4rmLOsAPxG5VERcgFuAc79sXgRiMbrELgEigSlKqQJgOJChlPKxbBmWOmOAr4EA4FPrxkSkNUYy+h8QYmk3+QLxfQZswEgm/8RIetVxwTis9AfigCuAKSJyqaV8KhAFxABXARfsclNKXW552MnyOXxped4cCAJaAxMx/q9/ZHneCjgNvHmR+HsBOzHe/0vAhyIiNdj3M2A9EAw8B9x+kWNqjYBOKpozVZytXAVsB9IrXrB8KU0EHlFKHVNKnQT+hZF8LmatUupbpVS5Uur0Oa+NB1YqpT5XSpUopY4qpc5LKiLSCugBPKuUKlJKrQYWVfO9XSyOCtOUUqeVUpuATUAnS/lNwL+UUseVUmnAjGoeG6AcmGqJ/7Tlvc5XSp2yfJYvAAMvUv+AUuoDpVQZ8DEQDoRVZ1+rz3GKUqpYKfUrsLAG70VrQHRfq+ZMnwCrgWjO6frCOJPwAjZY/YEsgEsVbR66yGstgT02xBUBHLecEVU4YKlvq4vFUeGI1eNTgI/V8a3r29LWubKVUoUVT0TEC3gNGAYEWop9RcTFkgwuGJtS6pTld+BTyX4X27cZcEwpdeqc91Kdz1FrYPSZiuY0SqkDGBfsRwDfnPNyDkYXTQelVIBl81dKVXyxXWjY4sWGMx4C2tgQ2mEg0HLtpkIrq8cFWA0qsHTfhVQjDluObz2qrSZfwuce/1GMrrZeSik/oKLb7EJdWvZwGAiyJLQKOqE0cjqpaM52DzDknLMClFLlwAfAayISCiAikSJytWWXTCBYRPyrcaxPgStF5CYRcRWRYBHpfO5OlmSXCEwTETcR6Q+MstplF+AhIteIiBl4BnCvRhxVmQdMtlxcjwQerGL/TIzrLxfji5Gkc0UkCOO6jUNZfY7PWT7HPpz9OWqNkE4qmlMppfYopRIv8PITQCqwzjKKaiXGX9sopXYAnwN7LSO5Imw41kGMs6JHgWMYF+k7XWD38RgXoI9hfAGf6Z5TSp0A/grMxLgOVABUOjqrhqZb2tuH8Z6/Boousv9zwMeWz+GmC+zzOuCJcQa4Dlhqt2gv7lagD3AUeB74kou/F62B0zc/alo9JyIPALcopS52Yb1BEJEvgR1KKYefKWnOoc9UNK2eEZFwEeknIiYRicM4s1rg7LhqQkR6iEgby3sZhjHU+ltnx6U5jsOSioi0FJFVIrJNRLaKyMOW8iARWSEiuy0/Ay3lIiIzRCRVRDaLSFertiZY9t8tes4hrfFzA94DTgI/Ad8Bbzs1opprDiQA+RhDox9QSm10akSaQzms+0tEwoFwpVSSiPhi3Eh2LXAnxjDDF0XkSSBQKfWEiIwA/obR590LeEMp1ctyUTER6I4xomUD0E0pddwhgWuapmk15rAzFaXUYaVUkuXxSYyb2yIxTn8/tuz2MUaiwVI+RxnWAQGWxHQ1sMJyA9xxYAXGWHtN0zStnqmTmx9FJAroAvwOhCmlDlteOsKfd+lGcvZNXmmWsguVV3aciRh3YePh4dGtVatWle1Wb5SXl2MyVT+vK+BgXjm+bkKQh223GRwrPcap8lO0cKv+pL41jbOu6TjtS8dpXw0hzl27duUopc6956paHJ5URMQHmA/8XSmVZz19kFJKiTFtt10opd4H3geIi4tTO3futFfTDpGQkMCgQYNqVPemd9dSVFrGdw/2t2n/ZfuX8Y+f/8Gc4XPoEtqlWseqTZx1ScdpXzpO+2oIcYrIgdq24dC0abkxbD7wqVKq4o7pTEu3VsV1lyxLeTpn323bwlJ2ofImrUd0ICkZeRQUldq0f+/w3pjExG/pvzk4Mk3TmjJHjv4S4ENgu1LqVauXFvLnjK8TMEa2VJTfYRkF1hs4YekmWwYMtdxdHAgMtZQ1aT2igigrVyQfyrVpf393fzo268iajDUOjkzTtKbMkWcq/TCmuR4iIsmWbQTGdOZXichu4ErLc4AlwF6MO6g/wLhjGaXUMYypx/+wbNMtZU1at9aBmATW77P9o+gf0Z+UnBRyC21LRJqmadXlsGsqlmmuL3QV+YpK9lfApAu0NQuYZb/oGj5fDzPtmvvxx37bk0rfyL68velt1h1ex7BoPYBOq72SkhLS0tIoLCysemcH8ff3Z/v27U47vq3qU5weHh60aNECs9ls97b11PcNWM/oIL744yDFpeW4uVZ90tkxuCN+bn78lvGbTiqaXaSlpeHr60tUVBQXXsPLsU6ePImvr69Tjl0d9SVOpRRHjx4lLS2N6Ohou7dfv8e3aRfVOyaYwpJyNqXZ1p3lYnKhd3hv1qSvQc/5ptlDYWEhwcHBTksoWvWJCMHBwQ47u9RJpQHrHROECKxJPWpznX6R/cg6ncXu3N0OjExrSnRCaXgc+TvTSaUBC/Byo324H2v35thcp19EPwB+SfvFUWFpmtaE6aTSwPWJCSbpYC6FJZWtCHu+MO8wLg26lNVpqx0cmaZpTZFOKg1cnzbBFJeWk3TA9vk1L29xOcnZyZwoOuHAyDStbuTm5vL22zWbxPn111/n1KlTdo7obFFRUeTk5JwXZ0ZGBjfccAMAycnJLFmypEbt79+/n44dO9olVnvQSaWB6xkdhItJWLvX9usqA1sMpFyV82v6rw6MTNPqxokTJ+p1UqlwbpwRERF8/fXXQO2SSn2jhxQ3cL4eZjpG+rN2j+1JpUOzDgR5BPFz2s9cE3ONA6PTmpJpi7ayLSPPrm22j/Bj6qgOF91n6tSp7Nmzh86dO3PVVVcRGhrKvHnzKCoq4rrrrmPatGkUFBRw0003kZaWRllZGc8++yyZmZlkZGQwePBgmjVrxqpVqypt38fHhwceeIAlS5YQHh7Ov/71Lx5//HEOHjzI66+/zujRo5k9ezaJiYm8+eabAIwcOZJ//OMfZ831dW6ckyZNYuTIkSQlJTFlyhROnz7Nr7/+yuTJk4mOjubhhx+msLAQT09PPvroI+Li4ti6dSt33XUXxcXFlJeXM3/+/LPuNdm7dy/XX38977//Pj169Kj9L6AGdFJpBPrEBDPzl72cKi7Fy63qX6lJTFze4nJ+PPgjpeWluJr0PwOt4Zo2bRo7d+4kOTmZ5cuX8/XXX7N+/XqUUowePZrVq1eTnZ1NREQE33//PWCcNfj7+/Pqq6+yatUqmjVrdsH2CwoKGDJkCC+//DLXXXcdzzzzDCtWrGDbtm1MmDCB0aNHVztOMLqtANzc3Jg+ffpZSSkvL49ffvkFV1dXVq5cyVNPPcX8+fN59913efjhh7n11lspLi6mrKyMzMxMAHbu3Mktt9zC7Nmz6dSpU00/zlrT3yaNQN82wbz78x7+2H+cgbG2zVo9sMVAvk39luSsZLo37+7gCLWmoKozirqwfPlyli9fTpcuxkzc+fn57N69mwEDBvDoo4/yxBNPMHLkSAYMGGBzm25ubgwbZtwsHB8fj7u7O2azmfj4+DOJwd5OnDjBhAkT2L17NyJCSUkJAH369OGFF14gLS2NsWPH0rZtWwCys7MZM2YM33zzDe3bt3dITLbS11Qage5RgZhdpFpdYH0i+uBqctWjwLRGRSnF5MmTSU5OJjk5mdTUVO655x5iY2NJSkoiPj6eZ555hunTp9vcptlsPnNfh8lkwt3d/czj0lJjlnBXV1fKy8vP1KntjYXPPvssgwcPJiUlhUWLFp1pb/z48SxcuBBPT09GjBjBTz/9BBhTwLRq1Ypff3X+dVKdVBoBLzdXOrUIqNbFem+zNz3CevBz2s8OjEzTHM/Hx4eTJ08CcPXVVzNr1izy8/MBSE9PJysri4yMDLy8vLjtttt47LHHSEpKAsDX1/dM3dqIiooiOTmZ8vJyDh06xPr16y8a57nOjePEiRNERhprEc6ePftM+d69e4mJieGhhx5izJgxbN68GTDOphYsWMCcOXP47LPPav1+akMnlUaiT5tgtqTlkldYYnOdgS0HsvfEXg7lHap6Z02rp4KDg+nXrx8dO3ZkxYoVjB8/nj59+hAfH88NN9zAyZMn2bJlCz179qRz585MmzaNZ555BoCJEycybNgwBg8eXKsY+vXrR3R0NO3bt+ehhx6ia9euF43zscceO+u1wYMHs23bNjp37syXX37J448/zuTJk+nSpcuZsyGAefPm0bFjRzp37kxKSgp33HHHmde8vb1ZvHgxr732GgsXLqzV+6kVpVSj3GJjY1V9t2rVKru19Vtqtmr9xGK1fOsRm+scPHFQdZzdUc3dNvei+9kzTkfScdqXLXFu27bN8YFUIS8vz9kh2KS+xVnZ7w5IVLX87tVnKo1Et9aBeLm5sHpXts11Wvq1JMY/hlWHKh9KqWmaVl169Fcj4e7qQt82wSTsykIpZfOEcUNaDeGjlI84UXQCf3d/B0epafVXr169KCoqOqvsk08+IT4+3kkRNUw6qTQiA2NDWLk9i305BcSE+NhU54pWVzBzy0x+TvuZ0W1sG2+vaY3R77//7uwQGgXd/dWIDIwNBeDnanSBdQjuQJhXGD8e+NFRYWma1oQ4LKmIyCwRyRKRFKuyL63Wq98vIsmW8igROW312rtWdbqJyBYRSRWRGaIXb7igVsFexDTzrlZSERGGtBrCmow1nC497cDoNE1rChx5pjIbOGvNWqXUzUqpzkqpzsB84Burl/dUvKaUut+q/B3gXqCtZdPr4F7E5bEhrNt71Oap8MHoAissK2RN+hoHRqZpWlPgsKSilFoNHKvsNcvZxk3A5xdrQ0TCAT+l1DrLcLc5wLX2jrUxGRgXQmFJOb/vq/Sjr1S3sG74u/uz8uBKB0amaVpT4KxrKgOATKWU9Zq20SKyUUR+FpGKiXkigTSrfdIsZdoF9I4Oxs3VxM87be8CczW5MrDFQH5O+5mScttvntS0+sAZ66lMmTKFlStr/keYj8/FB9LU5j3Bn2u4OIOzRn+N4+yzlMNAK6XUURHpBnwrItWenU5EJgITAUJCQkhISLBHrA6Tn5/vkBhjA4QfkvdzuW+WzXXCToVxsvgkHy77kHae7c56zVFx2puO075sidPf398u05zUxvHjx3nzzTe5/fbbq133tdde49prryU4OLha9SruiK/Oey8rKztr/4vVTUtLq/F7AuOm9vz8/DPzlFWmsLDQIf8O6zypiIgrMBboVlGmlCoCiiyPN4jIHiAWSAdaWFVvYSmrlFLqfeB9gLi4OGW9lkF9lJCQgCNi3OO6j38u3kaby3rSMsjLpjq9S3sz98u55ATmMKj32TE5Kk5703Haly1xbt++HV9fX+PJD0/CkS32DaJ5PAx/8aK7TJs2jX379jFgwIBqr6dy+PBhRo0adcH1VMrKyrjnnntITExERLj77rt55JFHuPPOOxk5ciQ33HADUVFRTJgwgUWLFlFSUsJXX31Fu3btyM7OZvz48WRkZNCnTx+WL19OUlLSmWn2Kz63l19++bx4n3/++bPe09SpUxkzZgzHjx+npKSE559/njFjxlT6vm6++WZEBB8fH1xdXRk7dixjx47l3nvvPeu9eXh4nJnN2Z6ccaZyJbBDKXWmW0tEQoBjSqkyEYnBuCC/Vyl1TETyRKQ38DtwB/A/J8TcoAyMDeGfwOrd2dzaq7VNdTxcPegX0Y+fDv7EU72ewiR6tLnWMDhyPZXk5GTS09NJSTEGsebm5la6X7NmzUhKSuLtt9/mlVdeYebMmUybNo0hQ4YwefJkli5dyocffnheveXLl7N79+7z4n3xxRdJSUk5s/ZKaWkpCxYswM/Pj5ycHHr37s3o0aNZunTpee+rQn5+Prfccgt33HHHWXOEOZrDkoqIfA4MApqJSBowVSn1IXAL51+gvxyYLiIlQDlwv1Kq4krzXzFGknkCP1g27SLahHjTItCTVTtsTyoAV7a+kpUHV5KclUzXsPMnxNO0i6rijKIu2Hs9lZiYGPbu3cvf/vY3rrnmGoYOHVrpfmPHjgWgW7dufPONMaj1119/ZcGCBQAMGzaMgIAAm+Nt1arVWfsppXjqqadYvXo1JpOJ9PR0MjMziY+Pv+D7GjNmDI8//ji33nqrTe/VXhyWVJRS4y5QfmclZfMxhhhXtn8i0NGuwTVyIsLguFC+3pBGYUkZHmYXm+oNajkIdxd3lu1fppOK1iApy3oq991333mvJSUlsWTJEp555hmuuOIKpkyZUmV7gYGBbNq0iWXLlvHuu+8yb948Zs2add5+FdcuXFxczppVuKbxnrv416effkp2djYbNmzAbDYTFRVFYWHhmXViKntf/fr1Y+nSpYwfP97maZvsQfdxNFJD2oVyuqSMddVcY2VA5ACWH1hOWbnt97lomjM5cj2VnJwcysvLuf7663n++efP1LNFv379mDdvHmCckVTWdXaheCtbXyU0NBSz2cyqVas4cOAAwAXfF8D06dMJDAxk0qRJNsdsD3rur0aqT5tgPMwmVu3IYlBcqM31ro6+mpUHV5KUlUSP5j0cGKGm2Yf1OiXDhw8/s54KGAln7ty5pKam8thjj2EymTCbzbzzzjvAn+upREREVHqhPj09nbvuuuvMqo7//ve/bY5r6tSpjBs3jk8++YQ+ffoQFhb256AGi6FDh7J9+/bz4m3Tps1Z7+mJJ55g1KhRxMfH0717d9q1M0ZobtmypdL3VeGNN97g7rvv5vHHH+ell16yOfZaqe3c+fV1a2rrqVTm7o/Wq/7/+VGVl5fbXKeguED1mNtD/XPtP8+UNab1P+qDxhSnXk/lwgoLC1VJSYlSSqk1a9ao+Ph4J0d0Nr2eilZtg9uFcujYaVKz8m2u42X24vIWl7PiwApKy23vG9Y07WwHDx6kR48edOrUiYceeogZM2Y4O6Q6obu/GrHB7Yxur592ZNE2zLeKvf90ddTVLNu/jMTMRHqH93ZUeJpWr9h7PZW2bduycePGM8+dfZNoXdFJpRGLDPCkXXNfftqRxX0D29hcb0DkADxdPVm2f5lOKlqToddTsQ/d/dXIDWkXSuKB45w4bfucXh6uHgxqOYiVB1bqucA0TasWnVQauSHtQikrV9Vaux5gWNQwcotyWX94vYMi0zStMdJJpZHr0iqQAC8zq3bYPrkkQP/I/vi6+fL93u8dFJmmaY2RTiqNnItJGBQbQsKubMrKlc313FzcGNp6KCsPrqSovKjqCpqmaeik0iQMbhfKsYJikg9VPhnehYxqM4rTpafZfGqzgyLTtNqr6dojI0aMuOAEkY5055138vXXXwPnr+firJjsSSeVJmBgbAgmodpdYF1CuxDhHcEfBX84KDJNq70TJ05UmlSqmoNryZIllU7yWJfOTSr1Iaba0kOKm4AALze6tw7ipx1Z/OPqOJvrmcTENTHXMHPLTHJO59DMs/LpwTUN4D/r/8OOYzvs2ma7oHY80fOJi+4zdepU9uzZQ+fOnTGbzXh4eBAYGMiOHTvYtWsX1157LYcOHaKwsJCHH36YiRMnAsbqiImJieTn5zN8+HD69+/PmjVriIyM5LvvvsPT07PS4w0aNIguXbrwyy+/UFBQwJw5c/j3v//Nli1buPnmm3n++efZv38/I0eOPDNl/iuvvMLRo0fPmuZlxowZZGRkMHjw4DPruVTEdKGp+BsCfabSRAxuF8q2w3kcPnG6WvVGxoxEoViyd4mDItO02pk2bRpt2rQhOTmZl19+maSkJN544w127doFwKxZs9iwYQOJiYnMmDGDo0fPn2R19+7dTJo0ia1btxIQEMD8+ZVOmn6Gm5sbiYmJ3H///YwZM4a33nqLlJQUZs+eXWn7lXnooYfOzDlW2bxjDZU+U2kihrQL5T9Ld7BqRzbje7WquoJFTEAMLd1asnjvYu7oUHcL/WgNT1VnFHWlZ8+eREdHn3k+Y8aMM+uaHDp0iN27d5+3fHB0dDSdO3cGjDVRzp16/lyjR48GID4+ng4dOhAeHg4Y668cOnSowXdh1YY+U2kiYsN8iAzw5MftmdWu29O7J9uPbWdP7h4HRKZp9uXt7X3mcUJCAitXrmTt2rVs2rSJLl26UFhYeF4d67XcbVkTpWJ/k8l0Vl2TyURpaSmurq5nZjYGKj1mY6WTShMhIgztEMYvqTnkF1Vvosiu3l1xERcW7VnkoOg0reas11M514kTJwgMDMTLy4sdO3awbt26OokpLCyMrKwsjh49SlFREYsXL650v6rWc2mIdFJpQoZ1aE5xaTkJO6s3CszPxY++EX1ZtHeRXrxLq3es11N57LHHznpt2LBhlJaWcumll/Lkk0/Su3fdzGVnNpuZMmUKPXv25Kqrrjqz/sm5KtZzGTx4cJ3EVSdqO3d+fd30eirnKy0rV12nL1cPfpZUrXqrVq1Sy/cvVx1nd1SrD612UHS115jWKakP9Hoq9lXf4mxw66mIyCwRyRKRFKuy50QkXUSSLdsIq9cmi0iqiOwUkautyodZylJF5ElHxdsUuJiEq9qHsWpHFkWl1TvjGNRiEIHugSxIXeCg6DRNawwc2f01GxhWSflrSqnOlm0JgIi0B24BOljqvC0iLiLiArwFDAfaA+Ms+2o1dHWH5uQXlbIm1fa16wHMLmZGthnJqkOrOFZ4zEHRaVr9MWnSJDp37nzW9tFHHzk7rHrPYUOKlVKrRSTKxt3HAF8opYqAfSKSCvS0vJaqlNoLICJfWPbdZudwm4y+lwTj4+7Ksq1HziziZauxl4zlk22fsHiPHl6s/UkphYg4Owy7e+utt5wdgsMYPV2O4Yz7VB4UkTuAROBRpdRxIBKwHpaRZikDOHROea8LNSwiE4GJACEhISQkJNgxbPvLz893SowdAhXfbzrE0KCjmGz4MrCOM8otirmb5tIyq2W9+yJx1udZXY0pTh8fH9LS0vD393fav4eysrIGMYKqvsSplOLEiRMUFBQ45N9hXSeVd4B/Asry87/A3fZqXCn1PvA+QFxcnBo0aJC9mnaIhIQEnBFjflAGD362Ee/Wl9ErJrjK/a3jzNmVw7S10wjuGMxlIZc5ONLqcdbnWV2NKc6SkhLS0tJIT0+vm6AqUVhYiIeHh9OOb6v6FKeHhwedOnXCbDbbve06TSpKqTN33onIB0DF4O10oKXVri0sZVykXKuhQXGhuLmaWLY106akYm1Y1DBe+uMlFqQuqHdJRat7ZrP5rLvXnSEhIYEuXbo4NQZbNJQ4a6tO71MRkXCrp9cBFSPDFgK3iIi7iEQDbYH1wB9AWxGJFhE3jIv5C+sy5sbIx92VAZc0Y9nWI9XuW/Vx8+Gq1lfxw74fOFVyquoKmqY1KY4cUvw5sBaIE5E0EbkHeElEtojIZmAw8AiAUmorMA/jAvxSYJJSqkwpVQo8CCwDtgPzLPtqtTQ8Ppz03NNsrOYaKwA3xt5IQUkBS/bpSSY1TTubI0d/jauk+MOL7P8C8EIl5UsA/e1lZ1d3COOpBSYWJmfQtVVgtep2CulEbGAsX+78kuvbXl/vLthrmuY8epqWJsrXw8yQuFC+33K4WssMgzGP2M1xN7Pj2A425+hVITVN+5NOKk3Y6M4RZJ8sYt3e6t0ICXBNzDV4uXoxb+c8B0SmaVpDpZNKEzakXSg+7q4sTM6odl1vszej2oxi6b6l5BY27DW1NU2zH51UmjAPswtD24fxQ8rhas8FBnBT3E0Ulxfzbeq3DohO07SGSCeVJm5U5wjyCktZvSun2nVjA2PpEtqFebvmUa7Kq66gaVqjp5NKE9f/kmYEeplZuKn6XWBgnK0cOnmItRlr7RyZpmkNkU4qTZzZxcSI+HBWbsvkVHH1VoQEGNp6KMEewczdPtcB0Wma1tDopKIxulMEp0vKWLGt+uvXu7m4cXO7m/k1/Vf25u51QHSapjUkOqlo9IgKItzfg0U17QKLvQk3k5s+W9E0TScVDUwmYeRl4fy8K5vcU8XVrh/sGczINiNZtGeRHl6saU2cTioaAKM7RVJSpliacqRG9W+79DYKywr5atdXdo5M07SGRCcVDYCOkX5EN/PmuxrcCAnQNrAtfcL78PmOzykpK7FzdJqmNRQ6qWiAMZ/XqE4RrNt3lMy8whq1cXv728k+nc3S/UvtHJ2maQ2FTiraGaM7RaAULN58uEb1+0X2I9o/mjnb5jh0DWxN0+ovnVS0My4J9aF9uF+Nb4Q0iYm7OtzFjmM7WJOxxs7RaZrWEOikop1ldOcINh3K5cDRghrVHxkzklCvUGZumWnnyDRNawh0UtHOMqpTBECN71kxu5iZ0H4CiZmJJGcl2zM0TdMaAJ1UtLNEBnjSMyqI+UnpNb4uckPsDfi7+/NhygUX+tQ0rSNg1IkAACAASURBVJFy5Br1s0QkS0RSrMpeFpEdIrJZRBaISIClPEpETotIsmV716pON8u69qkiMkP02rUOd0vPluzLKWDd3mM1qu9l9mJ8u/EkHEog9XiqnaPTNK0+c+SZymxg2DllK4COSqnLgF3AZKvX9iilOlu2+63K3wHuBdpatnPb1OxsRHw4fh6ufL7+YI3bGN9uPJ6unsxKmWXHyDRNq+8cllSUUquBY+eULVdKVUyFuw5ocbE2RCQc8FNKrVNGX8wc4FpHxKv9ycPswtiuLViacoTjBdWftgUgwCOAG2JvYMm+JRw6ecjOEWqaVl+5OvHYdwNfWj2PFpGNQB7wjFLqFyASSLPaJ81SVikRmQhMBAgJCSEhIcHeMdtVfn5+vY2xjZRTXFbOS1/9TL9mRTWKM640DhMmpi+bzq3NbrV/kOeoz5+nNR2nfek46xenJBUReRooBT61FB0GWimljopIN+BbEelQ3XaVUu8D7wPExcWpQYMG2Slix0hISKA+x/jNod9Yf7SEoa1daxznjvU7+HzH50zpOoWWfi3tG+A56vvnWUHHaV86zvqlzkd/icidwEjgVkuXFkqpIqXUUcvjDcAeIBZI5+wushaWMq0OjOvZij3ZBezOrflSwXd3vBtXkyvvbn636p01TWvw6jSpiMgw4HFgtFLqlFV5iIi4WB7HYFyQ36uUOgzkiUhvy6ivO4Dv6jLmpmzkZeH4uruScKj6K0JWCPEK4cbYG1m8dzEH8g7YMTpN0+ojRw4p/hxYC8SJSJqI3AO8CfgCK84ZOnw5sFlEkoGvgfuVUhUX+f8KzARSMc5gfnBUzNrZvNxcGdMlgj+OlHLiVM1nHr4n/h7cTG68t+k9O0anaVp95LBrKkqpcZUUV3o3nFJqPjD/Aq8lAh3tGJpWDbf0aMXcdQdZsDGNO/tF16iNZp7NuCnuJuZun8u9l91LtH/N2tE0rf7Td9RrF9Ux0p9oPxOfrz9Uq5mH7+p4F+4u7ryV/JYdo9M0rb7RSUWr0sCWruzMPMnGQzVfKriZZzPuaH8Hy/YvY2vOVjtGp2lafaKTilalXuGueLm58PnvNb/DHuDODncS6B7Iaxte0+utaFojpZOKViVPV2FM5wgWbc4gr7DmF+x93Hy4r9N9/H7kd9ZmrLVjhJqm1Rc6qWg2uaVHKwpLymu8hn2FG2NvJNInkteSXqNc1fz+F03T6iedVDSbXNbCn/bhfnz2+8FadV25ubjxty5/Y8exHfywT48O17TGRicVzSYiwrieLdl+OI9NaSdq1dbw6OFcGnQpM5JmUFhaaKcINU2rD3RS0Ww2pksk3m4uzFmzv1btmMTEYz0eI6Mgg4+3fmyf4DRNqxd0UtFs5udh5sbuLVm0OYOsvNqdYfRo3oOrWl/FhykfcqTgiJ0i1DTN2XRS0arlrn5RlJYr5qyt/Txej3Z/lLLyMl7b8JodItM0rT7QSUWrltbB3lx5aRif/n6AwpKyWrUV6RPJnR3vZMm+JWzM2minCDVNcyabk4qI9BeRuyyPQ0RET+DURN3TP5rjp0pYsLH2qxDc0/EeQr1CeXH9i5SV1y5JaZrmfDYlFRGZCjzBn2vKm4G5jgpKq996RQfRIcKPWb/uq/Wd8V5mLx7t9ijbjm5j/u5K5xTVNK0BsfVM5TpgNFAAoJTKwJjCXmuCRIS7+0WzOyuf1btzat3e8Ojh9Grei9c3vE7O6dq3p2ma89iaVIotqzQqABHxdlxIWkMwqlMEIb7ufPjrvlq3JSI83ftpCssKefmPl+0QnaZpzmJrUpknIu8BASJyL7AS+MBxYWn1nZuriQl9WrN6Vza7Mk/Wur1o/2j+Ev8XluxbwpqMNXaIUNM0Z7ApqSilXsFYkXE+EAdMUUr9z5GBafXfrb1a42E28eEvtT9bAWOFyNZ+rXlh3QsUlRXZpU1N0+qWrRfqvYGflFKPYZyheIqI2aGRafVeoLcbN3RrwYLkdLJP1j4JuLu483Svpzl48qBeeljTGihbu79WA+4iEgksBW4HZldVSURmiUiWiKRYlQWJyAoR2W35GWgpFxGZISKpIrJZRLpa1Zlg2X+3iEyozhvUHOvuftGUlJXb5doKQJ+IPoxuM5pZKbPYdnSbXdrUNK3u2JpURCl1ChgLvKOUuhHoYEO92cCwc8qeBH5USrUFfrQ8BxgOtLVsE4F3wEhCwFSgF9ATmFqRiDTniwnxYXSnCD5es98uZysAj/d4nCCPIJ797VlKymq+foumaXXP5qQiIn2AW4HvLWUuVVVSSq0Gjp1TPAaomEXwY+Baq/I5yrAOY1BAOHA1sEIpdUwpdRxYwfmJSnOih69oS3FZOe8k7LFLe/7u/kzpM4Vdx3cxc8tMu7SpaVrdcLVxv79j3Pi4QCm1VURigFU1PGaYUuqw5fERIMzyOBI4ZLVfmqXsQuXnEZGJGGc5hISEkJCQUMMQ60Z+fn69jxFsi7NvuAtz1u4j3nyEQA/7zP7T3bs77256F98sX1q4tbBLnPWBjtO+dJz1jFLKoRsQBaRYPc895/Xjlp+Lgf5W5T8C3YF/AM9YlT8L/KOq48bGxqr6btWqVc4OwSa2xHnwaIG65Knv1dMLNtvtuMdPH1cDvxiorv/uelVUWlTl/o3p86wPdJz21RDiBBJVLb/zbR391V1EvhGRJMtF9M0isrmGeSzT0q2F5WeWpTwdaGm1XwtL2YXKtXqkZZAXN/doyZd/HOLQsVN2aTPAI4Dp/aaz8/hOXt3wql3a1DTNsWztp/gU46L79cAoq60mFgIVI7gmAN9Zld9hGQXWGzihjG6yZcBQEQm0XKAfainT6pkHB7dFRJjx4267tXl5i8u57dLb+HT7pyQcSrBbu5qmOYatSSVbKbVQKbVPKXWgYquqkoh8DqwF4kQkTUTuAV4ErhKR3cCVlucAS4C9QCrGvTB/BVBKHQP+Cfxh2aZbyrR6prm/B7f1as03G9PZm51vt3Yf6fYIlwZdyrO/PUtmQabd2tU0zf5sTSpTRWSmiIwTkbEVW1WVlFLjlFLhSimzUqqFUupDpdRRpdQVSqm2SqkrKxKEpUtvklKqjVIqXimVaNXOLKXUJZbtoxq+V60OPDCoDW4uJt6w49mKm4sbL13+EkVlRTz5y5N6inxNq8dsTSp3AZ0xhvJWdH2NdFRQWsMV4uvOnf2iWLgpg51Haj8nWIUo/yie7vU0iZmJfLBFTzunafWVrUmlh1Kqu1JqglLqLst2t0Mj0xqs+y6PwcfNlddW7LJru6PbjGZE9Aje2fQOSZlJdm1b0zT7sDWprBGR9g6NRGs0ArzcuLt/NEu3HiEl/YTd2hURnu39LJE+kTy++nGOnj5qt7Y1TbOPKpOKiAgwEEgWkZ2W4cRbajGkWGsC7hkQjb+nmVftfLbi4+bDfwf+l9yiXP7x8z8oKdfTuGhafVJlUrHcEBOKMSfXUP68nlLTIcVaE+DnYea+gTH8tCOLDQeO27XtS4Mv5bm+z5GYmcirifr+FU2rT2zt/poPhFoPJ7ZlSLHWtE3oE0Wwtxuvrthp97ZHxozk9va3M3f7XBbtWWT39jVNqxlbk0ovYK2I7NHdX5qtvN1deWBQG35LPcraPfa//vF/3f6PHs17MG3tNLZkb7F7+5qmVZ+tSeVqoA0wBN39pVXDbb1bE+bnzsvLdlTM3WY3riZXXhn4CiGeITz404McLdUX7jXN2WxdTvhAZZujg6sNl7LTzg5BAzzMLjxyZSxJB3NZsuWI3dsP8gjirSvfoqS8hHez3iWvOM/ux9A0zXb2maO8HvI8nQkF+i/X+uDG7i1p19yXf/+wncIS+98NH+MfwxuD3yC7JJv/S/g/PSJM05yo0SYVUWXw7QNg5y4XrfpcTMIz17Qn7fhpZv6y1yHH6NG8B+OCx/H74d95bs1zlKtyhxxH07SLa7RJpcijGexeBmvfcnYoGtC/bTNGxDdnxk+p7MspcMgxevn0YlLnSSzcs5BXEl+x+zUcTdOq1miTSrHZH9qNhJVTIW2Ds8PRgKmjOuDuYuLpBVsc9oV/32X3ceult/LJtk/0HGGa5gSNNqkAMOZN8I2AL2+DvAxnR9Pkhfl58PjwdqzZc5TFmw9XXaEGRITHezzOyJiR/G/j//hyx5cOOY6maZVr3EnFMxDGfQZFefDZzVBkvzU+tJoZ37MVl4b78eIPOxxy0R7AJCam95vOoBaDeOH3F/g29VuHHEfTtPM17qQC0DwebvgIMlNg/l9Ar8XhVC4mYeqo9qTnnub91Y65aA9gNpl5ZdAr9A7vzZTfprB472KHHUvTtD81/qQCEDsUhr8Eu36A5c84O5omr3dMMCPim/NOwh4On3Dc/UTuLu68MeQNejTvwdO/Ps3S/UsddixN0wxNI6kA9LwXej0A696G9foCrrNNHn4pZUrxnx92OPQ4nq6e/G/I/+gc0pknVz/J93u/d+jxNK2pq/OkIiJxIpJsteWJyN9F5DkRSbcqH2FVZ7KIpFqm3r+6xge/+gWIHQ4/PA67ltnl/Wg10zLIi4kDYvg2OYMNB4459FheZi/evvJtuoZ15clfnuTT7Z869Hia1pTVeVJRSu1USnVWSnUGugGngAWWl1+reE0ptQTAsjjYLUAHjOWM3xYRlxod3OQC1880rrN8eTvsXlnr96PV3AOD2hDu78HTC1IoKXPszYreZm/eufIdrmh1BS+uf5EZSTP0fSya5gDO7v66AthTxTxiY4AvlFJFSql9QCrQs8ZHdPeB27+FkDj4Yhyk/ljjprTa8XZ3ZfqYjuw4cpIPHHSnvTV3F3f+O/C/XN/2ej7Y8gHT1k6jTA/c0DS7Emf+tSYis4AkpdSbIvIccCeQByQCjyqljovIm8A6pdRcS50PgR+UUl9X0t5EYCJASEhIt3nz5l3w2K4l+XTa9AxepzLY1Gk6ef7t7Pzuqpafn4+Pj0+dH7e6HB3nmxsL2ZRdxvP9PAnzrvnfObbGqZRice5iluctp5NXJ+4IvgM3k1uNj1td+vduXzpO+xk8ePAGpVT3WjWilHLKBrgBOUCY5XkY4IJx9vQCMMtS/iZwm1W9D4Ebqmo/NjZWVelkplJvdFbq3y2VOpJS9f52tmrVqjo/Zk04Os4jJ06rjlOWqvEfrFXl5eU1bqe6cc7ZOkd1nN1R3bLoFpVZkFnj41aX/r3bl47TfoBEVcvvdmd2fw3HOEvJBFBKZSqlypRS5cAH/NnFlQ60tKrXwlJWez6hRleY2Rs+uQ6O7bNLs1r1hPl58MTwdvyWepT5Sfb51dri9va38/qg19lzYg/jFo8jJSelzo6taY2VM5PKOODziiciEm712nVAxf/whcAtIuIuItFAW2C93aIIbA23L4CyYvh4lE4sTjK+Zyu6tw5k+qKtpOfW3Vo4V7S+grkj5mJ2MTPhhwl6yLGm1ZJTkoqIeANXAd9YFb9ktUzxYOARAKXUVmAesA1YCkxSStn36mpoO7jjOyjOh49GQNZ2uzavVc1kEv57UyfKFTzyRTJl5XV3rS82MJbPrvmM+JB4nvzlSV7f8Lq+gK9pNeSUpKKUKlBKBSulTliV3a6UildKXaaUGq2UOmz12gtKqTZKqTil1A8OCSq8E9z5PZSXwswrYfsihxxGu7DWwd7889oOrN9/jDd/Sq3TYwd5BPHBVR9wY+yNfJjyIfetvI+c0zl1GoOmNQbOHlJcv4R1gPt+NoYbf3kbrHnT2RE1Odd1acF1XSJ548ddJO537E2R5zK7mJnSZwrT+05nU9Ymrl94PWsy1tRpDJrW0Omkci6/CLhzCbQfA8ufhiWPQWmRs6NqUqaP6UCLQC8e/iKZE6frfmng69pexxcjvyDII4j7V9zPjKQZlJaX1nkcmtYQ6aRSGbOHMbNx70mw/n34YAgc3ePsqJoMXw8zM8Z1ITOvkKccuKDXxbQJaMNn13zG2LZj+WDLB9y97G7S8+tuZJqmNVQ6qVyIyQWG/QvGfWks8DXzSjj4u7OjajI6twzg/4bG8v3mw3yVmOaUGDxdPXmu73P8Z8B/2HV8F2O/G8u8nfP09C6adhE6qVQlbhj8ZSV4BsDHI+GXV6FMd4XUhfsvb0PfNsFMXbiVPdnOW2BtRMwIFoxewGUhl/HPdf9k4oqJZOTrlUQ1rTI6qdgiuA3csxJih8GP02DmFZC51dlRNXomk/DazZ3xMJv422cbKSp13jDfcJ9w3r/qfZ7t/Sybszdz7XfX8uGWDykpq/trPppWn+mkYivvYLj5E7hxNpxIg/cGwtq3QHeFOFSYnwcv39CJbYfz+PcSx669UhUR4aa4m/h2zLf0jejL60mvc8OiG/jjyB9OjUvT6hOdVKqrw3UwaT3EXg3LnjKWKC446uyoGrUr24dxd79oZq/Zz3fJzr9YHu4TzuuDX+fNIW9SVFbE3cvuZvIvk/V9LZqGTio14x0MN30CQ56BrQtgRhdY+zbou7AdZvKIdvSICuTJ+VvYmnGi6gp1YGDLgSwYs4B74+9l6f6ljPhmBG9ufJP8Yudd/9E0Z9NJpaZMJrj8MfjrWmjZA5ZNhllXwxE9KaEjmF1MvDW+KwFeZu6e/YdD17avDk9XTx7q+hDfjvmWAZEDeG/ze4z4ZgSfbPuE4rJiZ4enaXVOJ5XaComDW7+GsTONe1ne7Q/fTTKGIWt2Fernwaw7e5BfWMq9cxKdeuH+XK39WvPfQf/l82s+JzYolpf+eIlRC0bxbeq3+sZJrUnRScUeROCyG+GhJOgzCTbPg/91g5XTID/L2dE1KpeG+/H6LV1ISXf+hfvKdGzWkZlDZ/LeVe/h7+7Ps789y6gFo1iwewEl5XqkmNb46aRiT56BcPULlgv5w+DX1+C1jrDwIcjZ7ezoGo2rrC7cL9lyuOoKTtA3oi9fjvySGYNn4Ovmy5Q1Uxi1YBQfb/2YU+WnnB2epjmMq7MDaJSCouHGj2Dw07D2TUj+DJLmGCPGev8Voi83zm60GntyeDuSDh7nkS+Tae7vQddWgc4O6TwiwuBWgxnUchCr01YzK2UWryS+gpu4sXHdRsZfOp5o/2hnh6lpdqXPVByp2SUw6nV4ZCsMfALSEmHOaOO6y28z8CpwzvQjjYGbq4mZE7rT3N+Dv3ycSGpW/R1xJSIMbDmQj4d/zBcjv6CzV2fm757P6G9Hc//K+1m+f7m+qK81Gjqp1AWfEBg82UguY94y5hVb8Sw9/5gEs0dCyjdQdNLZUTY4zXzcmX1XT0wijP9gHUcKyp0dUpU6BHfg9ma3s/yG5fy181/ZfWw3j/78KFd8dQUvrn+RHcfq33UiTasOnVTqktkDutwG962GR7ayJ2YCHN8PX98FL8XA5+OMBFNSP4bLNgTRzbz5/N5elJUrXvqjkKy8QmeHZJNmns14oNMDLL9hOe9c+Q69wnsxb+c8blx0IzcuupEPt3yoZ0XWGiSdVJzFvwWHWo2Fh5KNFSd73AsZG40E83Jb+OY+2PqtTjA2aBvmy8d39yS/RHHvJxsoLKk/Q42r4mJyoX9kf14Z+AqrblrF5J6TMZvMvJ70OsPmD2P89+P5eOvHHCk44uxQNc0mTksqIrLfsiZ9sogkWsqCRGSFiOy2/Ay0lIuIzBCRVBHZLCJdnRW33bm4QlR/Y5r9R7bCHd9B+9Gwayl8NQHe7gOpK6G8/nftOFPHSH/uu8ydzWm53DsnkfyihndviL+7P+MvHc9n13zGD2N/4O9d/05peSmvJL7CVV9fxfULr+fVxFf5/fDv+hqMVm85e/TXYKWU9YRJTwI/KqVeFJEnLc+fAIYDbS1bL+Ady8/GxeQCMYOMrawU9q6CH56AudeDuz9EdoXIbtBxrLH0sXaWbmGu/GdsHJMXbGHc++uYdWcPQnzdnR1WjbTwbcE98fdwT/w97D+xnx8P/siajDV8sv0TPtr6EZ6unvRs3pO+EX3pH9mfVn6tnB2ypgHOTyrnGgMMsjz+GEjASCpjgDnKWB1pnYgEiEi4Uqp+3qRgDy6u0PYq4yxm6wI4tB7SE417X375rzGxZYfrIHqAcX+MBsBNPVrSzNeNv36axPXvrGHO3T2Jaubt7LBqJco/6kyCOVVyivVH1vNb+m/8lvEbP6f9DEALnxb0i+xH34i+dAvrhr+7v5Oj1poqcdYqdiKyDzgOKOA9pdT7IpKrlAqwvC7AcaVUgIgsBl5USv1qee1H4AmlVOI5bU4EJgKEhIR0mzdvXh2+o+rLz8/Hx8enWnVcS07S8tACItO/x7WskHIxkxXaj2NBXTnt2ZyTvm1B7NurWZM4ncE6zj25Zby2oRAReKSbBzH+Lk6O7k/2/DyzS7LZXrid7ae3s6twF8WqGEEIN4fTxr0N0e7RtHZvTYhrCFLNe6Ma4u+9PmsIcQ4ePHiDUqp7bdpwZlKJVEqli0gosAL4G7CwIqlY9jmulAq0NalYi4uLUzt37nTwu6idhIQEBg0aVLPKpcWQvgFS5sOmL6DYMiQ5qA10uRWaxUHLXsZwZmfGWYfOjXNvdj53zFrP0fxi/jeuC1e2D3NecFYc9XkWlxWzKXsTSZlJbMjcQHJ2MqdLjYEefm5+xDeLp2OzjlwWchkdm3UkyCPIKXHam47TfkSk1knFad1fSql0y88sEVkA9AQyK7q1RCQcqJg4Kx1oaVW9haWs6XJ1g9Z9jG3o85B7ADKSYf178ON0Yx9xMe7e928Bbt7GFhRjJJuA1kYbjVhMiA/f/LUv98xO5C9zEvnbkEv4+5WxuJga52wGbi5u9Gjegx7NewBQWl7Kntw9pOSksCVnC1tytvDBlg8oV8agj0ifSNoHt+eSgEuICYihjX8bovyiMLuYnfk2tAbOKUlFRLwBk1LqpOXxUGA6sBCYALxo+fmdpcpC4EER+QLjAv2JRn09pbrMHsZsySFx0OlmOHXMmDF55/ewcylk74DiU1CcD8pquK1fpHHNJnogtO4Lrh7g6g5eF/8LtiEJ9fXgq/v7MOW7FP73UyrJh3J545YuBHk37oQK4GpyJS4ojrigOK6PvR6AUyWn2HZ0Gyk5KWzO2czOYzv58eCPZxKNi7jQyq8VlwRcgkuuC4X7CokJiCHKLwo3l8b/mWm156wzlTBggaWP1xX4TCm1VET+AOaJyD3AAeAmy/5LgBFAKnAKuKvuQ25AvIKMrWUPuPK5P8vLyyFnl3HBPy8DsrZD6o+w+cuz64e0g6gBxiCAVn3rMnKH8DC78NINnejaKpApC7dyzYxfeP3mzvSKCXZ2aHXOy+xF9+bd6d78zx6OwtJC9uftZ0/unjPbruO7OJh3kKWrlwJGsmnp25I2AW2Mzb8N0f7RhHuH4+/uX+3rNVrj5ZSkopTaC3SqpPwocEUl5QqYVAehNW4mE4S2M7YKSkHWNmN0GQpOH4f9v0Lyp/DHBwD0dg+GHRHg3wrC2hvDmcM6Gl1pJhsugJcWQXmp0f0GxgqZ6UlQdAKaxYJvhDHazcFu6dmKDhH+/O3zJMZ9sI4HB1/CQ1e0xdWlad8D7OHqQbugdrQLandW+YpVK2jVqRV7cveQmpvK3hN72ZO7h1WHVp05swFjobLm3s0J9w6nuXfzM48rtjDvMNxdGubQbq366tuQYq2uiViShNV9LwMeNQYCZGyEQ7+Tu/knmvu5w9FU2PUDVHyhuHoYZzUhcVCQAyePGNdpfCOM9spLjTOjPaugpAD8W4KLG5zKgUKrJYHFBN6h4Nsc/CKMrVmcMchgbwKczjXay9wKB9cZ7br7GgkpJBaC24J3M/xz90NmKHgGgIc/mL3Omw06voU/ix8awNTvtjLjp1R+Tc3hjVu60DLIy+EfdUNjFvOZ7jNrxWXF7DuxjwN5BzhScIQjp45wpOAIh/MPs/PYTo4WHj2vrSCPoDNJxjoBhXuHE+4TTpBHECY7j1rUnEMnFa1yrm7Qqhe06sWOkstoXjFqpeQ0ZO80vuCztkFminFm490MAltDWTEc3Q07lxhnMb4RcNlNRqI4mmpJCH5G15pPmLHOTF4GnDxsbLkH4cBvfyYdN1+jK2/bt8b+MYPAzQdOH4PsXUbSKSsCoAtAstV7MLkayeWczcfdj//6enPfZeX8sDOPuW/MY1iXNnRuE4m4+xjt+zY37v8pKzHeh6sHmD1tOzNr5Nxc3CpNNhWKy4rJLMjkcMFhDhccNhKO5efeE3v5LeO3M6PSKphN5jNnOWFeYYR4hhDiZWyhnqHGY88QPFw96uItarWgk4pWPWZPiOhsbBdTWgQms9HldjFR/c8vU8pIMHmHoXm8keCKToLZ+/z2yssgLx1OHWPTugQ6xbaEwjwjKVW25R2GojwoPkVscT6xYhm4kMzZCelCXNyMz8DV0xggYfayJBwv47n1Y7OXsa+bt5Go3LzB3ZfAY/shzcdIWh6Ws6o66P6rK24ubrT0a0lLv5aVvq6UIq8476xkY52ANmZtJPtUNsXl509F4+fmR6hXKKFeoYR4hhDqFUruyVzKDpYR6hlKM89mBHoE6uTjRI3nX7JWv7jWog9d5M9usAruvpXva3KBgFYQ0IrjQbnQcZDtx1EKyoopLczn63U7+XT1NjxUIRO6BjGstcK16ISR0MrLofQ0lBRCySkoLTTO2EpOW8otr50+blVu2bfktHH2ZqUTwOZzYqlIOm7ef3bbiQmw/Kx4XmmZ/FlmcjESmdkb3LwsbfqCu4+R8CraOHcze4KHn3EWafYEpfDO3w9Hthjdnarc+LyUsnR/KuOss6zEshVDeYnxmosbuLgbn52LO7iYjX8PljJxccff1Q1/n5bEBbSt9A+PisSTeSqTnFM5ZJ3OIvtUNlmnssg6lUX26Wz25O4h53QOZaqMr1Z9dVZ9T1dPgjyCCHQPJNDD2II8ggj0CMTPzQ9/d3/83PwIcA8gyCOIAI8AzCYHDqUut3x+TYBOKlrTJQKu7rj6c2XYTgAAEhVJREFUuHPLlX25vHsXpny3lQd/z6TdAV/+NXaEfVaULC02rikV5UNRHknrVtP10hjjWlFhrpGMik4aQ76L8o1EVPHFfebLvLySMmWcqVmXlZdaEluBZRh5gZH4aqAHwAVvL7YjF7c/z/wqEo+rB/4ubvi7mImtSFzlJZbE6WokKpOZMpdI9uXlURwUQJYojoniGOUck3KOF5VwvDCdnNz97C4v4bgqoYgLT8zqh4kgJfiXg68CP6XwL7dsCvyV4C8u+OKKp7jgZXLBU1yNzWTGxeRqJHWT65+bCBzbB1nbGaTK4Gf4848C00X+YOD8JF7xO3Z1N/7IOtMVK1bXDi0/z/yhYfUHx3llFT8t9frYZyyUTiqaZhER4MnMCd1ZtvUIU7/byti313BNfDiPDo0lJqQW02u4uhmbZY62PP9siB1kn6D/v717j5Grug84/v3Nnffau35iG4PZ5WEcGxpwgOAACU7TEKMEGiol0CiAqESoQppAqwiC1CJVSKRNU6VKVEoKSmgJj5AQaMUraV1ekbHB2MbGMRg/hLGN8Wu9u/Oe+fWPc2Y9O95dr9nrvbPL7yNd3Ttn7tz57Zm79zfn3DvnjkS14hJWpTAwQTVOpT7XLVg45JKQxFj/5kbOWnTWIC0bOXwgDFK+ZRJ3c8S1WupTpTj4vL5cKQ5s/TWWV4sumaTa3bZjgYu13kKqlQmqFWZWinT0HWJhQznViluv4UINBfK1EocKB+kWOBTEOBBPsD81iQPJNPsTSQ4EAd1x2E+NbdQ4pBV6qKL9W6j46UgphUxNyNaErEJGcfNshuzHLqSSKzGrfQpZCcgQkJXDU4aYWybmnkPIxpJkYglisaaEUCm6LyH1JOOjO9wS0sPljfP+5NS4fsM6yXDGyLOkYkyTyxbN5qLTZ3DvC1v49xe38MyG3XzlvJO47lOdLJjdHnV4xy6IuyvijtHeD6bCwkvDjydkr49w+BMBskC2WmF2Je/O+cVTR1wh2Kxaq9Jb7qW72E13sZueUg/5Sp5cJUe+kj+8XG4qK7vl7kqOXDnHwXieaqGXXDmH1hPBCKSDNNlElkw8QypIkY6nSafSpIIUqXiKTJBxZX7KxDNkggyZeGZgmZ/SQZpMws/9OmFeeWdJxZhBTErFufVP5nPtklP48f9u5sFXtvPQync5a247N33mNJadNWfCDvcy4QVxCIY4RzfY6rGAjlTHqEd+ro/9paoUqoX+pJPzSaeekPrLB3m+WC1SqBQoVov0VfrYV9hHsVokX8lTqBQoVAqDXuBwNKkgxS2fuGVUf1+dJRVjhjFjUoo7r1jEtz57Ov+1dicPrNjOzb94nRM7NnLFOXO5avFc5s8a+QHKGBHpbzWMlKpSriqlao1S5fBUVSUZj1GrKcVKlXypRm+pSE8xT3exj95ijt5Sjr5ynmK1QLFaoFwrUKoVKdUKlGtFyrUCZS1CaU4of58lFWNGYPqkFNdf1MXXl3Ty3Ibd/PK1Hfz0xS3c8/w7LJg9mc/Mn8nFZ8zg/M5ppBP2W5aJKF+qUqxUKVVrVKpKpaqUazUK5SrFSo1iuUahUqVYrlH080KlSrWm1BTWby7x+9xGegpl8qUqhfp6lYZtNCSMAQmkGsadX+PAJD8d6dKZp4TwHpZUjDkmQUxYdvYclp09h729Rf577U6e2bCb+1/eyr+9sIVUPMbSM09g2dmzWXLqdE5ot99LREFVKZRr9JUq5EtV+koV+opVcn6eL1fcQb1cpdCQEOoH90L5cHLIlaps3dvHru7CqOPKbN/OpHScbDIgHQ9IJWKk4jGyyThTszFSiRjJIEYy7qcgaFgWP4+RjLvyIAalSs21fhIB6URAJhGQScZIxQMySVeWiscIRIjFhCAmBCKIuP05JkJMXAvqGyHUvSUVYz6kGb71cv1FXfQVK6zcup/lm/bw1Bu7eWbDbgC6ZrRxQec0Luhy00lTR97l8VFVKFc5lC9zMF/mYK7MwVyJg/ky3bkyuVK1vzVQTwLv7izwwLZVdOfdut35Cofy5WP+dp8IhFQ8IJ1wB+RUIkbaP76gaxrzZ00mkwhIxGMkYkIiiBEPpP+g3fjadCJGypfHY4IgrFrxEp/77NLjVGutw5KKMSFoS8VZuuAEli44gb/70iLW7TjIqm37Wbl1P0+v38Ujr74LwIkdaeZly+xIb+djcybTOb2NaW3JCTfKr6qSL1d9UihzMF+iO9eQKOqP/fLBXNknhTL5cnXYbScCafiWH1At15geK9CRSXDm7Ml0ZBK0ZxK0pxNMSrlWQVvTvP6t3k1uO8f7wov4R+TCDksqxoQsiAnnzpvKufOmcuOnT6NWU97a08PKrft5Zet+Xtq0mxW/Wd+/fns6TteMNrpmtNHp510z2jhlWhvtmXikCadWU3qK7pt/YwJwLYfDyy4pND43fEshGY8xNZtgSiZJRzbBvGlZpmQTTMkm6cgk3HImyZRsov9xRyZBNhk/4uDvrqq65HhXhRkhSyrGHGexmLBgdjsLZrdz7ZJOli9fzml/9Ek2f9DD1r05tu7tZdveHKu2HeCJtTsHjObRlgw4cUqGWe3p/m/d2VRAWzLe31+ejsdIJ1wfeyJwE0C5OvCEb7nacCK4WqPcdDK4WKlxqHC4xbCvJ0f+2aeoDfOTirZkMCARnDFrEh0+GUzpTwb+cUOisIsZJi5LKsaMMRFh3vQs86YfOdx+oVxl+74cW/f2seNAjvcO5tl5MM/7h4rs6SmQK7kTx33FCsXKh78iKBG4cwL1E7+JwJ0kbk8nmNaWpGtGG737Syw8vZOOjGslTM0eTg4dGZdIknEbrt4MZEnFmBaSTgScOXsyZ84++m9fqv63CYWyu1qpXK351omiKCnfculPHPUrh4IYsRH077tupcGHtzdmKJZUjBmngpiQTcbJ2q3jTQsZ87ariJwsIstF5E0R2SAi3/bld4rIeyKyxk+XN7zmdhHZLCKbROSysY7ZGGPMyETRUqkAf62qq0VkMvCaiPzWP/fPqvqDxpVFZCFwNbAIOBH4nYjMV9Xhrzs0xhgz5sa8paKqu1R1tV/uATYCc4d5yZXAw6paVNWtwGbgguMfqTHGmGMV6aUbItKJu7X4K77oZhFZJyL3i0j97khzgXcbXraD4ZOQMcaYiIhGdItLEZmEuw/aXar6axGZBezF3XHm74E5qnqDiPwYWKGq/+lfdx/wtKo+Nsg2bwRuBJg5c+YnHn300TH6az6c3t5eJk0axc2fxojFGS6LM1wWZ3iWLl36mqqeN6qNqOqYT0ACeBa4dYjnO4H1fvl24PaG554FlhztPebPn6+tbvny5VGHMCIWZ7gsznBZnOEBXtVRHt+juPpLgPuAjar6w4byxsH8vwzUx7F4ErhaRFIi0gWcAawcq3iNMcaMXBRXf10EfB14Q0TW+LLvAdeIyDm47q9t4EZhVtUNIvIo8CbuyrFvql35ZYwxLWnMk4qqvoS7XXSzp4Z5zV3AXcctKGOMMaGwgXuMMcaExpKKMcaY0FhSMcYYExpLKsYYY0JjScUYY0xoLKkYY4wJjSUVY4wxobGkYowxJjSWVIwxxoTGkooxxpjQWFIxxhgTGksqxhhjQmNJxRhjTGgsqRhjjAmNJRVjjDGhsaRijDEmNJZUjDHGhMaSijHGmNBYUjHGGBOacZNUROQLIrJJRDaLyG1Rx2OMMeZI4yKpiEgA/ARYBiwErhGRhdFGZYwxptm4SCrABcBmVd2iqiXgYeDKiGMyxhjTJB51ACM0F3i34fEO4JPNK4nIjcCN/mFRRNaPQWyjMQPYG3UQI2BxhsviDJfFGZ4zR7uB8ZJURkRV7wXuBRCRV1X1vIhDGtZ4iBEszrBZnOGyOMMjIq+OdhvjpfvrPeDkhscn+TJjjDEtZLwklVXAGSLSJSJJ4GrgyYhjMsYY02RcdH+pakVEbgaeBQLgflXdcJSX3Xv8Ixu18RAjWJxhszjDZXGGZ9QxiqqGEYgxxhgzbrq/jDHGjAOWVIwxxoRmwiWVVh3ORUROFpHlIvKmiGwQkW/78jtF5D0RWeOny1sg1m0i8oaP51VfNk1Efisib/v51IhjPLOhztaIyCER+U4r1KeI3C8iexp/JzVU/YnzL35/XSciiyOM8R9F5A8+jsdFZIov7xSRfEOd3jMWMQ4T55CfsYjc7utyk4hcFnGcjzTEuE1E1vjyKOtzqONQePunqk6YCXcS/x3gVCAJrAUWRh2Xj20OsNgvTwbewg05cyfwN1HH1xTrNmBGU9k/ALf55duA70cdZ9Pnvhs4pRXqE/g0sBhYf7T6Ay4HngYEuBB4JcIYPw/E/fL3G2LsbFyvBepy0M/Y/z+tBVJAlz8WBFHF2fT8PwF/2wL1OdRxKLT9c6K1VFp2OBdV3aWqq/1yD7ARN1LAeHEl8HO//HPgTyOMpdkfA++o6vaoAwFQ1ReA/U3FQ9XflcAD6qwApojInChiVNXnVLXiH67A/R4sUkPU5VCuBB5W1aKqbgU2444Jx91wcYqIAF8BHhqLWIYzzHEotP1zoiWVwYZzabkDt4h0AucCr/iim33T8v6ou5U8BZ4TkdfEDX0DMEtVd/nl3cCsaEIb1NUM/IdttfqEoeuvVffZG3DfUOu6ROR1EXleRC6JKqgGg33GrVqXlwDvq+rbDWWR12fTcSi0/XOiJZWWJyKTgF8B31HVQ8C/AqcB5wC7cM3kqF2sqotxo0J/U0Q+3fikunZxS1yLLu7HsFcAv/RFrVifA7RS/Q1GRO4AKsCDvmgXME9VzwVuBX4hIu1Rxcc4+IybXMPALz2R1+cgx6F+o90/J1pSaenhXEQkgfsgH1TVXwOo6vuqWlXVGvBTxqi5PhxVfc/P9wCP42J6v97s9fM90UU4wDJgtaq+D61Zn95Q9ddS+6yIXA98EfiaP7jgu5P2+eXXcOcq5kcV4zCfcUvVJYCIxIGrgEfqZVHX52DHIULcPydaUmnZ4Vx8v+p9wEZV/WFDeWP/5JeBSEdWFpE2EZlcX8advF2Pq8fr/GrXAU9EE+ERBnwLbLX6bDBU/T0JXOuvsrkQ6G7ohhhTIvIF4LvAFaqaayifKe6eRojIqcAZwJYoYvQxDPUZPwlcLSIpEenCxblyrONr8jngD6q6o14QZX0OdRwizP0ziisQjueEu1rhLVz2vyPqeBriuhjXpFwHrPHT5cB/AG/48ieBORHHeSruCpq1wIZ6HQLTgf8B3gZ+B0xrgTptA/YBHQ1lkdcnLsntAsq4Pui/GKr+cFfV/MTvr28A50UY42Zc/3l9/7zHr/tnfl9YA6wGvhRxXQ75GQN3+LrcBCyLMk5f/jPgpqZ1o6zPoY5Doe2fNkyLMcaY0Ey07i9jjDERsqRijDEmNJZUjDHGhMaSijHGmNBYUjHGGBMaSyrGDEJEev28U0T+PORtf6/p8e/D3L4xUbKkYszwOoFjSir+V9TDGZBUVPVTxxiTMS3Lkooxw7sbuMTf9+IWEQnE3XdklR/Q8BsAInKpiLwoIk8Cb/qy3/hBOTfUB+YUkbuBjN/eg76s3ioSv+314u5n89WGbf+fiDwm7n4nD/pfRiMid4u7N8Y6EfnBmNeOMU2O9o3KmI+623D37vgigE8O3ap6voikgJdF5Dm/7mLgLHXDrgPcoKr7RSQDrBKRX6nqbSJys6qeM8h7XYUbJPHjwAz/mhf8c+cCi4CdwMvARSKyETdMyQJVVfE31TImStZSMebYfB43FtIa3JDh03FjNwGsbEgoAH8lImtx9yY5uWG9oVwMPKRusMT3geeB8xu2vUPdIIprcN1y3UABuE9ErgJyg2zTmDFlScWYYyPAt1T1HD91qWq9pdLXv5LIpbjBBJeo6seB14H0KN632LBcxd2hsYIbofcx3MjCz4xi+8aEwpKKMcPrwd12te5Z4C/98OGIyHw/mnOzDuCAquZEZAHuVqx15frrm7wIfNWft5mJu0XtkKPs+ntidKjqU8AtuG4zYyJl51SMGd46oOq7sX4G/AjX9bTanyz/gMFvrfwMcJM/77EJ1wVWdy+wTkRWq+rXGsofB5bgRohW4LuqutsnpcFMBp4QkTSuBXXrh/sTjQmPjVJsjDEmNNb9ZYwxJjSWVIwxxoTGkooxxpjQWFIxxhgTGksqxhhjQmNJxRhjTGgsqRhjjAnN/wNaPkaKRo++zwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evals_result = {}\n",
    "evals_result['test_multitask'] = evals_result_mt['valid_1']\n",
    "evals_result['test_singletask'] = evals_result_singlelgb['valid_1']\n",
    "evals_result['train_multi'] = evals_result_mt['training']\n",
    "\n",
    "ax = lgb.plot_metric(evals_result, metric='rmse',ylim = (0,2000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(942, 22) (31, 22)\n",
      "[0]\tvalidation_0-rmse:5875.17773\tvalidation_0-rmse:5875.17761\n",
      "[1]\tvalidation_0-rmse:5695.03613\tvalidation_0-rmse:5695.03591\n",
      "[2]\tvalidation_0-rmse:5519.79688\tvalidation_0-rmse:5519.79655\n",
      "[3]\tvalidation_0-rmse:5350.89502\tvalidation_0-rmse:5350.89506\n",
      "[4]\tvalidation_0-rmse:5186.15234\tvalidation_0-rmse:5186.15233\n",
      "[5]\tvalidation_0-rmse:5027.33936\tvalidation_0-rmse:5027.33926\n",
      "[6]\tvalidation_0-rmse:4872.51270\tvalidation_0-rmse:4872.51249\n",
      "[7]\tvalidation_0-rmse:4721.35840\tvalidation_0-rmse:4721.35849\n",
      "[8]\tvalidation_0-rmse:4576.19092\tvalidation_0-rmse:4576.19121\n",
      "[9]\tvalidation_0-rmse:4434.04150\tvalidation_0-rmse:4434.04172\n",
      "[10]\tvalidation_0-rmse:4298.22900\tvalidation_0-rmse:4298.22894\n",
      "[11]\tvalidation_0-rmse:4165.65674\tvalidation_0-rmse:4165.65649\n",
      "[12]\tvalidation_0-rmse:4037.68530\tvalidation_0-rmse:4037.68510\n",
      "[13]\tvalidation_0-rmse:3911.56079\tvalidation_0-rmse:3911.56094\n",
      "[14]\tvalidation_0-rmse:3791.75855\tvalidation_0-rmse:3791.75869\n",
      "[15]\tvalidation_0-rmse:3676.58203\tvalidation_0-rmse:3676.58184\n",
      "[16]\tvalidation_0-rmse:3563.55078\tvalidation_0-rmse:3563.55090\n",
      "[17]\tvalidation_0-rmse:3453.41772\tvalidation_0-rmse:3453.41787\n",
      "[18]\tvalidation_0-rmse:3345.25049\tvalidation_0-rmse:3345.25051\n",
      "[19]\tvalidation_0-rmse:3241.99145\tvalidation_0-rmse:3241.99152\n",
      "[20]\tvalidation_0-rmse:3140.33252\tvalidation_0-rmse:3140.33279\n",
      "[21]\tvalidation_0-rmse:3043.16211\tvalidation_0-rmse:3043.16217\n",
      "[22]\tvalidation_0-rmse:2947.69238\tvalidation_0-rmse:2947.69232\n",
      "[23]\tvalidation_0-rmse:2855.48193\tvalidation_0-rmse:2855.48183\n",
      "[24]\tvalidation_0-rmse:2765.87891\tvalidation_0-rmse:2765.87879\n",
      "[25]\tvalidation_0-rmse:2678.50757\tvalidation_0-rmse:2678.50761\n",
      "[26]\tvalidation_0-rmse:2594.49561\tvalidation_0-rmse:2594.49571\n",
      "[27]\tvalidation_0-rmse:2510.19116\tvalidation_0-rmse:2510.19133\n",
      "[28]\tvalidation_0-rmse:2430.98706\tvalidation_0-rmse:2430.98725\n",
      "[29]\tvalidation_0-rmse:2351.82861\tvalidation_0-rmse:2351.82854\n",
      "[30]\tvalidation_0-rmse:2277.37231\tvalidation_0-rmse:2277.37206\n",
      "[31]\tvalidation_0-rmse:2206.40088\tvalidation_0-rmse:2206.40080\n",
      "[32]\tvalidation_0-rmse:2135.15601\tvalidation_0-rmse:2135.15597\n",
      "[33]\tvalidation_0-rmse:2066.87158\tvalidation_0-rmse:2066.87167\n",
      "[34]\tvalidation_0-rmse:1998.87012\tvalidation_0-rmse:1998.87018\n",
      "[35]\tvalidation_0-rmse:1934.29883\tvalidation_0-rmse:1934.29890\n",
      "[36]\tvalidation_0-rmse:1873.16040\tvalidation_0-rmse:1873.16050\n",
      "[37]\tvalidation_0-rmse:1812.30872\tvalidation_0-rmse:1812.30875\n",
      "[38]\tvalidation_0-rmse:1754.34875\tvalidation_0-rmse:1754.34881\n",
      "[39]\tvalidation_0-rmse:1697.52283\tvalidation_0-rmse:1697.52268\n",
      "[40]\tvalidation_0-rmse:1638.60339\tvalidation_0-rmse:1638.60337\n",
      "[41]\tvalidation_0-rmse:1585.87488\tvalidation_0-rmse:1585.87484\n",
      "[42]\tvalidation_0-rmse:1534.48621\tvalidation_0-rmse:1534.48623\n",
      "[43]\tvalidation_0-rmse:1484.47742\tvalidation_0-rmse:1484.47743\n",
      "[44]\tvalidation_0-rmse:1436.41272\tvalidation_0-rmse:1436.41272\n",
      "[45]\tvalidation_0-rmse:1390.89087\tvalidation_0-rmse:1390.89086\n",
      "[46]\tvalidation_0-rmse:1343.80200\tvalidation_0-rmse:1343.80184\n",
      "[47]\tvalidation_0-rmse:1299.44104\tvalidation_0-rmse:1299.44103\n",
      "[48]\tvalidation_0-rmse:1253.16467\tvalidation_0-rmse:1253.16469\n",
      "[49]\tvalidation_0-rmse:1211.39026\tvalidation_0-rmse:1211.39026\n",
      "[50]\tvalidation_0-rmse:1172.05066\tvalidation_0-rmse:1172.05066\n",
      "[51]\tvalidation_0-rmse:1133.94019\tvalidation_0-rmse:1133.94014\n",
      "[52]\tvalidation_0-rmse:1098.12915\tvalidation_0-rmse:1098.12909\n",
      "[53]\tvalidation_0-rmse:1061.77771\tvalidation_0-rmse:1061.77770\n",
      "[54]\tvalidation_0-rmse:1028.25122\tvalidation_0-rmse:1028.25125\n",
      "[55]\tvalidation_0-rmse:994.12775\tvalidation_0-rmse:994.12779\n",
      "[56]\tvalidation_0-rmse:963.01569\tvalidation_0-rmse:963.01567\n",
      "[57]\tvalidation_0-rmse:932.06531\tvalidation_0-rmse:932.06531\n",
      "[58]\tvalidation_0-rmse:901.59033\tvalidation_0-rmse:901.59033\n",
      "[59]\tvalidation_0-rmse:871.60266\tvalidation_0-rmse:871.60269\n",
      "[60]\tvalidation_0-rmse:844.25983\tvalidation_0-rmse:844.25973\n",
      "[61]\tvalidation_0-rmse:815.15192\tvalidation_0-rmse:815.15190\n",
      "[62]\tvalidation_0-rmse:789.83997\tvalidation_0-rmse:789.83994\n",
      "[63]\tvalidation_0-rmse:765.39239\tvalidation_0-rmse:765.39242\n",
      "[64]\tvalidation_0-rmse:739.81128\tvalidation_0-rmse:739.81129\n",
      "[65]\tvalidation_0-rmse:716.44281\tvalidation_0-rmse:716.44280\n",
      "[66]\tvalidation_0-rmse:693.50000\tvalidation_0-rmse:693.49995\n",
      "[67]\tvalidation_0-rmse:671.18957\tvalidation_0-rmse:671.18952\n",
      "[68]\tvalidation_0-rmse:651.12408\tvalidation_0-rmse:651.12410\n",
      "[69]\tvalidation_0-rmse:631.31586\tvalidation_0-rmse:631.31586\n",
      "[70]\tvalidation_0-rmse:612.78857\tvalidation_0-rmse:612.78858\n",
      "[71]\tvalidation_0-rmse:594.59186\tvalidation_0-rmse:594.59190\n",
      "[72]\tvalidation_0-rmse:577.45007\tvalidation_0-rmse:577.45011\n",
      "[73]\tvalidation_0-rmse:559.29571\tvalidation_0-rmse:559.29574\n",
      "[74]\tvalidation_0-rmse:544.93335\tvalidation_0-rmse:544.93334\n",
      "[75]\tvalidation_0-rmse:529.58630\tvalidation_0-rmse:529.58631\n",
      "[76]\tvalidation_0-rmse:511.99597\tvalidation_0-rmse:511.99599\n",
      "[77]\tvalidation_0-rmse:498.75482\tvalidation_0-rmse:498.75482\n",
      "[78]\tvalidation_0-rmse:485.13791\tvalidation_0-rmse:485.13785\n",
      "[79]\tvalidation_0-rmse:472.03757\tvalidation_0-rmse:472.03755\n",
      "[80]\tvalidation_0-rmse:457.32535\tvalidation_0-rmse:457.32534\n",
      "[81]\tvalidation_0-rmse:446.26953\tvalidation_0-rmse:446.26957\n",
      "[82]\tvalidation_0-rmse:434.82666\tvalidation_0-rmse:434.82664\n",
      "[83]\tvalidation_0-rmse:426.15259\tvalidation_0-rmse:426.15257\n",
      "[84]\tvalidation_0-rmse:415.87253\tvalidation_0-rmse:415.87252\n",
      "[85]\tvalidation_0-rmse:407.29599\tvalidation_0-rmse:407.29600\n",
      "[86]\tvalidation_0-rmse:398.38315\tvalidation_0-rmse:398.38316\n",
      "[87]\tvalidation_0-rmse:391.93741\tvalidation_0-rmse:391.93739\n",
      "[88]\tvalidation_0-rmse:381.97494\tvalidation_0-rmse:381.97495\n",
      "[89]\tvalidation_0-rmse:375.01877\tvalidation_0-rmse:375.01877\n",
      "[90]\tvalidation_0-rmse:369.17889\tvalidation_0-rmse:369.17888\n",
      "[91]\tvalidation_0-rmse:362.93811\tvalidation_0-rmse:362.93814\n",
      "[92]\tvalidation_0-rmse:356.66113\tvalidation_0-rmse:356.66112\n",
      "[93]\tvalidation_0-rmse:352.60696\tvalidation_0-rmse:352.60698\n",
      "[94]\tvalidation_0-rmse:348.21350\tvalidation_0-rmse:348.21350\n",
      "[95]\tvalidation_0-rmse:342.96759\tvalidation_0-rmse:342.96758\n",
      "[96]\tvalidation_0-rmse:339.39450\tvalidation_0-rmse:339.39449\n",
      "[97]\tvalidation_0-rmse:335.65881\tvalidation_0-rmse:335.65882\n",
      "[98]\tvalidation_0-rmse:332.93133\tvalidation_0-rmse:332.93135\n",
      "[99]\tvalidation_0-rmse:330.40692\tvalidation_0-rmse:330.40688\n",
      "[100]\tvalidation_0-rmse:328.23178\tvalidation_0-rmse:328.23177\n",
      "[101]\tvalidation_0-rmse:326.14841\tvalidation_0-rmse:326.14838\n",
      "[102]\tvalidation_0-rmse:323.04413\tvalidation_0-rmse:323.04413\n",
      "[103]\tvalidation_0-rmse:321.11102\tvalidation_0-rmse:321.11101\n",
      "[104]\tvalidation_0-rmse:319.19763\tvalidation_0-rmse:319.19764\n",
      "[105]\tvalidation_0-rmse:319.30475\tvalidation_0-rmse:319.30473\n",
      "[106]\tvalidation_0-rmse:318.50476\tvalidation_0-rmse:318.50476\n",
      "[107]\tvalidation_0-rmse:317.65543\tvalidation_0-rmse:317.65542\n",
      "[108]\tvalidation_0-rmse:317.63858\tvalidation_0-rmse:317.63858\n",
      "[109]\tvalidation_0-rmse:317.49994\tvalidation_0-rmse:317.49994\n",
      "[110]\tvalidation_0-rmse:317.17941\tvalidation_0-rmse:317.17943\n",
      "[111]\tvalidation_0-rmse:316.54239\tvalidation_0-rmse:316.54237\n",
      "[112]\tvalidation_0-rmse:316.50720\tvalidation_0-rmse:316.50720\n",
      "[113]\tvalidation_0-rmse:316.68619\tvalidation_0-rmse:316.68620\n",
      "[114]\tvalidation_0-rmse:317.10422\tvalidation_0-rmse:317.10423\n",
      "[115]\tvalidation_0-rmse:317.46967\tvalidation_0-rmse:317.46966\n",
      "[116]\tvalidation_0-rmse:316.43927\tvalidation_0-rmse:316.43930\n",
      "[117]\tvalidation_0-rmse:317.15393\tvalidation_0-rmse:317.15393\n",
      "[118]\tvalidation_0-rmse:317.65689\tvalidation_0-rmse:317.65688\n",
      "[119]\tvalidation_0-rmse:318.45056\tvalidation_0-rmse:318.45056\n",
      "[120]\tvalidation_0-rmse:319.50055\tvalidation_0-rmse:319.50058\n",
      "[121]\tvalidation_0-rmse:320.40543\tvalidation_0-rmse:320.40543\n",
      "[122]\tvalidation_0-rmse:320.63858\tvalidation_0-rmse:320.63857\n",
      "[123]\tvalidation_0-rmse:321.51205\tvalidation_0-rmse:321.51205\n",
      "[124]\tvalidation_0-rmse:322.27420\tvalidation_0-rmse:322.27421\n",
      "[125]\tvalidation_0-rmse:323.38669\tvalidation_0-rmse:323.38668\n",
      "[126]\tvalidation_0-rmse:324.75073\tvalidation_0-rmse:324.75070\n",
      "[127]\tvalidation_0-rmse:326.31180\tvalidation_0-rmse:326.31179\n",
      "[128]\tvalidation_0-rmse:326.67844\tvalidation_0-rmse:326.67843\n",
      "[129]\tvalidation_0-rmse:328.53122\tvalidation_0-rmse:328.53123\n",
      "[130]\tvalidation_0-rmse:330.32016\tvalidation_0-rmse:330.32016\n",
      "[131]\tvalidation_0-rmse:331.32541\tvalidation_0-rmse:331.32540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[132]\tvalidation_0-rmse:333.38583\tvalidation_0-rmse:333.38586\n",
      "[133]\tvalidation_0-rmse:332.93445\tvalidation_0-rmse:332.93446\n",
      "[134]\tvalidation_0-rmse:335.27643\tvalidation_0-rmse:335.27641\n",
      "[135]\tvalidation_0-rmse:337.04837\tvalidation_0-rmse:337.04838\n",
      "[136]\tvalidation_0-rmse:337.92270\tvalidation_0-rmse:337.92270\n",
      "[137]\tvalidation_0-rmse:340.16708\tvalidation_0-rmse:340.16711\n",
      "[138]\tvalidation_0-rmse:342.77698\tvalidation_0-rmse:342.77697\n",
      "[139]\tvalidation_0-rmse:344.99463\tvalidation_0-rmse:344.99462\n",
      "[140]\tvalidation_0-rmse:345.69949\tvalidation_0-rmse:345.69950\n",
      "[141]\tvalidation_0-rmse:348.23062\tvalidation_0-rmse:348.23061\n",
      "[142]\tvalidation_0-rmse:349.12918\tvalidation_0-rmse:349.12917\n",
      "[143]\tvalidation_0-rmse:349.86261\tvalidation_0-rmse:349.86261\n",
      "[144]\tvalidation_0-rmse:352.99084\tvalidation_0-rmse:352.99086\n",
      "[145]\tvalidation_0-rmse:353.05707\tvalidation_0-rmse:353.05706\n",
      "[146]\tvalidation_0-rmse:355.96195\tvalidation_0-rmse:355.96197\n",
      "[147]\tvalidation_0-rmse:356.66861\tvalidation_0-rmse:356.66859\n",
      "[148]\tvalidation_0-rmse:358.22153\tvalidation_0-rmse:358.22154\n",
      "[149]\tvalidation_0-rmse:359.70532\tvalidation_0-rmse:359.70532\n",
      "[150]\tvalidation_0-rmse:362.49890\tvalidation_0-rmse:362.49888\n",
      "[151]\tvalidation_0-rmse:363.25101\tvalidation_0-rmse:363.25101\n",
      "[152]\tvalidation_0-rmse:364.58765\tvalidation_0-rmse:364.58765\n",
      "[153]\tvalidation_0-rmse:367.43491\tvalidation_0-rmse:367.43490\n",
      "[154]\tvalidation_0-rmse:369.84064\tvalidation_0-rmse:369.84063\n",
      "[155]\tvalidation_0-rmse:370.41962\tvalidation_0-rmse:370.41961\n",
      "[156]\tvalidation_0-rmse:372.98248\tvalidation_0-rmse:372.98249\n",
      "[157]\tvalidation_0-rmse:373.45837\tvalidation_0-rmse:373.45839\n",
      "[158]\tvalidation_0-rmse:376.04425\tvalidation_0-rmse:376.04425\n",
      "[159]\tvalidation_0-rmse:377.20538\tvalidation_0-rmse:377.20539\n",
      "[160]\tvalidation_0-rmse:379.25922\tvalidation_0-rmse:379.25919\n",
      "[161]\tvalidation_0-rmse:379.37054\tvalidation_0-rmse:379.37055\n",
      "[162]\tvalidation_0-rmse:379.33810\tvalidation_0-rmse:379.33812\n",
      "[163]\tvalidation_0-rmse:379.88534\tvalidation_0-rmse:379.88532\n",
      "[164]\tvalidation_0-rmse:384.02072\tvalidation_0-rmse:384.02071\n",
      "[165]\tvalidation_0-rmse:384.30960\tvalidation_0-rmse:384.30957\n",
      "[166]\tvalidation_0-rmse:384.69311\tvalidation_0-rmse:384.69312\n",
      "[167]\tvalidation_0-rmse:385.04681\tvalidation_0-rmse:385.04681\n",
      "[168]\tvalidation_0-rmse:385.48651\tvalidation_0-rmse:385.48650\n",
      "[169]\tvalidation_0-rmse:387.31909\tvalidation_0-rmse:387.31909\n",
      "[170]\tvalidation_0-rmse:388.26971\tvalidation_0-rmse:388.26972\n",
      "[171]\tvalidation_0-rmse:388.76211\tvalidation_0-rmse:388.76212\n",
      "[172]\tvalidation_0-rmse:389.18448\tvalidation_0-rmse:389.18450\n",
      "[173]\tvalidation_0-rmse:388.67297\tvalidation_0-rmse:388.67296\n",
      "[174]\tvalidation_0-rmse:389.11481\tvalidation_0-rmse:389.11478\n",
      "[175]\tvalidation_0-rmse:390.37653\tvalidation_0-rmse:390.37654\n",
      "[176]\tvalidation_0-rmse:390.77377\tvalidation_0-rmse:390.77375\n",
      "[177]\tvalidation_0-rmse:391.02106\tvalidation_0-rmse:391.02106\n",
      "[178]\tvalidation_0-rmse:391.33548\tvalidation_0-rmse:391.33545\n",
      "[179]\tvalidation_0-rmse:394.36249\tvalidation_0-rmse:394.36248\n",
      "[180]\tvalidation_0-rmse:397.41244\tvalidation_0-rmse:397.41245\n",
      "[181]\tvalidation_0-rmse:398.75116\tvalidation_0-rmse:398.75118\n",
      "[182]\tvalidation_0-rmse:399.01834\tvalidation_0-rmse:399.01835\n",
      "[183]\tvalidation_0-rmse:399.71545\tvalidation_0-rmse:399.71546\n",
      "[184]\tvalidation_0-rmse:400.09293\tvalidation_0-rmse:400.09290\n",
      "[185]\tvalidation_0-rmse:401.03876\tvalidation_0-rmse:401.03877\n",
      "[186]\tvalidation_0-rmse:400.84265\tvalidation_0-rmse:400.84263\n",
      "[187]\tvalidation_0-rmse:401.11130\tvalidation_0-rmse:401.11131\n",
      "[188]\tvalidation_0-rmse:401.29089\tvalidation_0-rmse:401.29089\n",
      "[189]\tvalidation_0-rmse:402.48352\tvalidation_0-rmse:402.48352\n",
      "[190]\tvalidation_0-rmse:402.81189\tvalidation_0-rmse:402.81189\n",
      "[191]\tvalidation_0-rmse:403.30472\tvalidation_0-rmse:403.30475\n",
      "[192]\tvalidation_0-rmse:403.19583\tvalidation_0-rmse:403.19585\n",
      "[193]\tvalidation_0-rmse:404.97024\tvalidation_0-rmse:404.97025\n",
      "[194]\tvalidation_0-rmse:405.24036\tvalidation_0-rmse:405.24034\n",
      "[195]\tvalidation_0-rmse:405.67255\tvalidation_0-rmse:405.67251\n",
      "[196]\tvalidation_0-rmse:406.59235\tvalidation_0-rmse:406.59236\n",
      "[197]\tvalidation_0-rmse:406.86856\tvalidation_0-rmse:406.86857\n",
      "[198]\tvalidation_0-rmse:406.99368\tvalidation_0-rmse:406.99369\n",
      "[199]\tvalidation_0-rmse:407.48770\tvalidation_0-rmse:407.48771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.03, max_delta_step=0,\n",
       "             max_depth=6, min_child_weight=1, missing=None, n_estimators=200,\n",
       "             n_jobs=1, num_leaves=48, num_parallel_tree=1,\n",
       "             objective='reg:squarederror', random_state=0, reg_alpha=0.1,\n",
       "             reg_lambda=0.2, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='auto', verbosity=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "startdate = \"20191015\"\n",
    "features = [x for x in df_feature.columns if x not in ['report_date','amount','amount2']]\n",
    "\n",
    "params = {\n",
    "        \"n_estimators\":200,\n",
    "    \"max_depth\":6,\n",
    "    \"learning_rate\":0.03,\n",
    "            'reg_alpha' :0.1,\n",
    "        'reg_lambda' : 0.2,\n",
    "    'num_leaves':48,\n",
    "}\n",
    "\n",
    "\n",
    "target = 'amount'\n",
    "X_train = df_feature[df_feature['report_date']<startdate][features]\n",
    "y_train = df_feature[df_feature['report_date']<startdate][target]\n",
    "X_valid = df_feature[df_feature['report_date']>=startdate][features]\n",
    "y_valid = df_feature[df_feature['report_date']>=startdate][target]\n",
    "\n",
    "print(X_train.shape,X_valid.shape)\n",
    "evals_result = {}\n",
    "clf = xgb.XGBRegressor(\n",
    "    **params\n",
    ")\n",
    "#     clf.fit(X_train, y_train)\n",
    "def xgb_mape(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return('mape', np.mean(np.abs((labels - preds) / (labels+0.0001))))\n",
    "def xgb_rmse(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return('rmse', np.mean((labels-preds) ** 2)**0.5)    \n",
    "clf.fit(X_train, y_train, eval_set = [(X_valid,y_valid)],eval_metric = xgb_rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
